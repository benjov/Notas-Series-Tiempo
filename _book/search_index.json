[["index.html", "Notas de Clase: Series de Tiempo Capítulo 1 Introducción 1.1 a) La naturaleza de los datos de Series de Tiempo 1.2 b) Ejemplos y aplicaciones de las Series de Tiempo", " Notas de Clase: Series de Tiempo Benjamín Oliva &amp; Omar Alfaro-Rivera 2021-08-30 Capítulo 1 Introducción Estas notas son un resumen, una síntesis comparativa y, en algunos casos, una interpretación propia de los libros de texto de Cowpertwait y Metcalfe (2009), Guerrero-Guzmán (2014), Enders (2015), Franses y van Dijk (2003), Kirchgassner, Wolters, y Hassler (2012), Lutkepohl (2005), Wei (2019), entre otros. En algunos casos se incorpora información adicional para efectos de dar contexto al tema analizado (ver sección de Bibliografía para mayores detalles). El objetivo de este documento es proporcionar un conjunto de apuntes que sirva de apoyo para la clase, por ello no deben considerarse como notas exhaustivas o como un sustituto de la clase y los laboratorios. Asimismo, es deseable que los alumnos puedan aportar sus observaciones y correcciones a estas notas, las observaciones a estas notas son esperadas y siempre serán bienvenidas y agradecidas. En estas notas se estudian los temas que típicamente son incluidos como parte de un curso estándar de análisis de series de tiempo y agrega otros tantos, los cuales son: Modelos estacionarios univaraidos: AR(p), MA(q), ARMA(p, q) y ARIMA( p, d, q); Modelos no estacionarios univariados y Pruebas de raíz unitaria (o pruebas para determinar que una serie es estacionaria); Modelos multivariados, entre lo que se incluye a los Vectores Autoregresivos (VAR) y los procedimientos de Cointegración Modelación de series univariadas con errores con heterocedasticidad y autocorrelación: ARCH(r), GARCH(n), etc.; Modelos multivariados con errores con heterocedasticidad y autocorrelación: M-GARCH y M-GARCH-M; Casos particulares en los que las series incluidas en un modelo multivariado no son del mismo orden de integración, conocidos como modelos ADL. Modelos de Datos Panel en series de tiempo, y Modelos no lineales como los de cambios de régimen. 1.1 a) La naturaleza de los datos de Series de Tiempo El análisis de series de tiempo tiene muchas aplicaciones en diversos campos de la ciencia. Por ejemplo, en la economía continuamente se está expuesto a observaciones de los mercados financieros, indicadores de empleo, índices o indicadores del nivel de producción, índices de precios, etc. En otros campos de las ciencias sociales se emplea el análisis de series de tiempo para analizar la evolución de la población, los nacimientos, o el número de personas con matriculas escolares. Finalmente, en las ciencias exactas se pueden encontrar casos como los de un epidemiólogo que puede estar interesado en el número de casos de influenza observados en algún periodo de tiempo dado y si a estos se les puede asociar con algún tipo de estacionalidad o si se trata del inicio de un fenómeno atípico. La primera aproximación que se suele tener a las series de tiempo es mediante el exámen de datos puestos en una gráfica, en la cual uno de los ejes es el tiempo y el otro es el valor tomado por la variable. No obstante, en este tipo de exámenes existen dos enfoques. Por un lado, existe el efoque de la importancia del tiempo, el cual consiste en reconocer cómo lo que sucede hoy es afectado por lo que pasó ayer o, en general, en periodos pasados, o cómo lo que pasa hoy afectará los eventos futuros. Por otro lado, existe el enfoque del análisis frecuentista o de frecuencia, mediante el cual se busca reconocer la importancia que tiene para los investigadores los ciclos (estacionales, de crisis económicas, etc.) Figure 1.1: Indicador Global de Actividad Económica (IGAE) Global y para las Actividades Primarias (2008=100), Ene.2002 - May.2021 1.2 b) Ejemplos y aplicaciones de las Series de Tiempo Un primer ejemplo que puede ilustrar la presencia de los dos tipos de enfoques antes mencionadas es la Figura 1.1. En esta figura se muestra la evolución del Indicador Global de la Actividad Económica (IGAE) en su versión global o del total de la economía y en su versión únicamente para las actividades primarias entre enero de 2002 y mayo de 2021. Como se puede observar, el IGAE del total de la economía muestra, principalmente, que el enfoque del tiempo es más relevante. Es decir, que existe cierta persistencia en el indicador, lo que significa que la economía crece en razón del crecimiento reportado en periodos pasados. No obstante, lo que no podemos reconocer es que los eventos futuros tienen un efecto en el desempeño de la economía hoy día. Así, no es común observar cambios abruptos del indicador, salvo por la crisis global de 2008 y la reciente crisis causada por la Covid-19. Figure 1.2: índice de Confianza del Consumidor (ICC): General y resultado de ¿Cómo considera usted la situación economica del país hoy en día comparada con la de hace 12 meses? (puntos), Ene.2002-may.2021 Por el contrario, el IGAE de las actividades primarias muestra una presencia significativa de la importancia de la frecuencia. No pasa desapercibido que existen muchos ciclos en la evolución del indicador. Algo que suena común en las actividades primarias, cuya producción depende de eventos que son ciclícos agrícolas asociados con el clima u otros factores determinantes de la oferta de productos agrícolas. Otro factor que puede incluir en el indicador son elementos de demanda, más que los de oferta. Por ejemplo, el consumo de alimentos típicos de algunas temporadas del año. Como segundo ejemplo, en la Figura 1.2 se ilustra la evolución reciente del índice de Confianza del Consumidor (ICC) en dos de sus versiones: i) el Índice global y ii) el Índice de confianza de los consumidores cuando estos consideran la situación actual en la economía en relación el año anterior. Destacamos que el ICC mide las expectativas de los consumidores en razón de la información pasada y de la esperada, segun dichos consumidores. Figure 1.3: índice de Precios y Cotizaciones de la Bolsa Mexicana de Valores (Panel Derecho) y Tipo de Cambio para Solventar Obligaciones en Moneda Extranjera, pesos por dólar (Panel izquierdo), Ene.2002-May.2021 Así, es probable que las dos series de tiempo exhiban un gran peso para los eventos pasados, pero a la vez, un componente -probablemente menor- del componente de frecuencia. Esto último en razón de que los consumidores suelen considerar en sus expectativas de consumo los periódos cíclicos de la economía: temporadas navideñaas, pagos de colegiaturas, etc. Este sengundo ejemplo tambien ilustra que la confianza del consumidor no necesariamente está directamente correlacionada con el desempeño de la economía. Como tercer ejemplo se muestra la evolución de dos series. La Figura 1.3 ilustra el comportamiento reciente de dos indicadores que son referencia para los inversionistas. Por un lado, se ubica el índice de Precios y Cotizaciones de la BMV (IPC), el cuál refleja el valor de las acciones de empresas que cotizan en la BMV y el volumen de acciones comercializadas, en conjunto. De esta forma, se ha interpretado que el IPC refleja el rendimiento del capital promedio invertido en las empresas que cotizan en la BMV. Por otro lado, en la Figura 1.3 se presenta la evolución del Tipo de Cambio (TDC){indicador financiero que se suele utilizar como medio de reserva de valor. Esto, en razón de que el TDC es conocido como un instrumento que en momentos de crisis toma valores contraciclicos de la economía mexicana. No obstante, ambos indicadores no son comparables. Para hacerlos comparbles en la Figura 1.4 se presentan en versión índice con una base en el primer mes de la muestra. Figure 1.4: Índice del índice de Precios y Cotizaciones de la Bolsa Mexicana de Valores (Panel Derecho) e Índice del Tipo de Cambio para Solventar Obligaciones en Moneda Extranjera (ambos enero de 2002 = 100), pesos por dólar (Panel izquierdo), Ene.2002-May.2021 En la perspectiva de la Figura 1.4 se puede apreciar que el TDC no es tan rentable, ya que una inversión en la BMV mediante el IPC, en el largo plazo, muestra más redimientos. Asimismo, la Figura 1.4 ilustra que en ambas series se observa un dominio de la condición de tiempo y no uno de frecuencia. Es decir, tanto el IPC como el TDC no responden a condiciones como ciclos o temporadas que si son observables en actividades económicas como las primarias. Finalmente, la Figura 1.5 ilustra un característica que también resulta de gran interés en el analásis de series de tiempo: los datos de alta frecuencia y de comportamiento no regular. Como se puede observar, en la Figura 1.5 se muestran las diferencias logarítmicas de las series de IGAE de la actividad total, el IPC y el TDC. Figure 1.5: Tasas de Crecimiento mensuales (diferencias logarítmicas) de Indicador Global de la Actividad Económica, Índice de Precios y Cotizaciones de la Bolsa Mexicana de Valores (Panel Derecho) y Tipo de Cambio para Solventar Obligaciones en Moneda Extranjera, Ene.2002-May.2021 Dichas diferencia se pueden interpretar como una tasa de crecimiento de las series por las siguientes razones. Consideremos una serie de tiempo dada por \\(y_t\\), cuya versión logarítmica es \\(ln(y_t\\)). De esta forma, la diferencia logarítmica esta dada por la ecuación (1.1): \\[\\begin{equation} \\Delta ln(y_t) = ln(y_t) - ln(y_{t-1}) = ln \\left( \\frac{y_t}{y_{t-1}} \\right) \\tag{1.1} \\end{equation}\\] Ahora bien, si retomamos la definición de tasa de crecimiento (TC) de una serie de tiempo \\(y_t\\) entre el periodo \\(t\\) y \\(t-1\\) podemos obtener que: \\[\\begin{equation} TC = \\frac{y_t - y_{t-1}}{y_{t-1}} = \\frac{y_t}{y_{t-1}} - 1 \\tag{1.2} \\end{equation}\\] De esta forma, si tomamos el logarítmo de la expresión de la ecuación (1.2) obtenemos la siguiente aproximación: \\[\\begin{equation} \\frac{y_t}{y_{t-1}} -1 \\approx ln \\left( \\frac{y_t}{y_{t-1}} \\right) = ln(y_t) - ln(y_{t-1}) \\tag{1.3} \\end{equation}\\] La ecuación (1.3) es cierta cuando los valores de \\(y_t\\) y \\(y_{t-1}\\) son muy parecidos, es decir, cuando las variaciones no son tan abruptas. Otra forma de interpretar la ecuación (1.3) es que para tasas de crecimiento pequeñas, se puede utilizar como una buena aproximación a la diferencia logarítmica mostrada en la ecuación (1.1). En la Figura 1.5 se reportan las diferencias logarítmicas del IGAE, IPC y TDC, todos, como una media de distitntos tipos de redimientos. Es decir, podemos decir que un capitalista promedio (suponiendo que solo puede invertir en la actividad económica, en la bolsa o en el dólar), puede observar que le es más redituable en función de sus preferencias. Notése que la dinámica de las variaciones de cada una de las series es significativamente diferente. Destaca que el TDC es una de las variables que, en general, no muestra grandes cambios a lo largo del tiempo. No obstante, se han observado cambios radicales, cuando menos en el año 2008. Lo anterior, son caracteristicas que se han observado para el IPC. En cambio, el IGAE muestra un comportamiento más estable o estacionario. "],["elementos-de-ecuaciones-en-diferencia.html", "Capítulo 2 Elementos de Ecuaciones en Diferencia 2.1 a) Ecuaciones en Diferencia para procesos deterministas 2.2 Operador de rezago L", " Capítulo 2 Elementos de Ecuaciones en Diferencia 2.1 a) Ecuaciones en Diferencia para procesos deterministas En el capítulo previo se hizó una introducción al concepto de series de tiempo. En este Capítulo se pretende desarrollar la construcción de los procesos generadores de datos de las series de tiempo. En un sentido más formal, se expondrá que las series de tiempo se pueden considerar como una secuencia de variables aleatorias. Para tales efectos, se desarrollará una introducción al concepto de ecuaciones en diferencia. Así, las preguntas que se pretende responder son: ¿Cuál es la solución de la ecuación en diferencia que se estudia? b)¿Cuáles son las condiciones para que un proceso estocástico, representado mediante una ecuación en diferencia, llegue a alcanzar un punto de equilibrio en el largo plazo? El término de ecuación en diferencia sirve para denominar un proceso similar o equivalente dentro de las ecuaciones diferenciales, dentro del cual se consideran a un conjunto de variables que están en función del tiempo. Así, si consideramos al tiempo como una variable continua, es decir, consideramos una variable \\(Z(t)\\), podemos expresar las siguientes expresiones para la ecuación diferencial: \\[\\begin{equation} \\frac{dZ(t)}{dt}; \\frac{d^2Z(t)}{dt^2}; \\ldots; \\frac{d^kZ(t)}{dt^k} \\tag{2.1} \\end{equation}\\] Por otro lado, suponiendo el caso del tiempo en forma discreta, es decir, con \\(t = \\ldots, -2, -1, 0, 1, 2, \\ldots\\), entonces el comportamiento de la serie de variables dadas por \\(Z_t\\), la cual se puede expresar como: \\[\\begin{equation} \\Delta Z_t; \\Delta^2 Z_t; \\ldots; \\Delta^k Z_t \\tag{2.2} \\end{equation}\\] Observemos que una forma técnicamente más correcta es escribir las expresiones anteriores como: \\[\\begin{equation} \\frac{\\Delta Z_t}{\\Delta t}; \\frac{\\Delta^2 Z_t}{\\Delta t^2}; \\ldots; \\frac{\\Delta^k Z_t}{\\Delta t^k} \\tag{2.3} \\end{equation}\\] No obstante, no pasa desapercibido que \\(\\Delta t = 1\\), por lo que resultan equivalentes ambos conjuntos de expresiones (2.2) y (2.3). 2.1.1 Ecuaciones en Diferencia Lineales de Primer Orden El primer caso que se suele estudiar en relación a Ecuaciones en Diferencia es el de las Ecuaciones en Diferencia Lineales de Primer Orden. Al respecto, al igual que en el caso continúo, las variaciones de la variable \\(Z_t\\) se pueden expresar como se ilustra en el siguiente ejemplo. Consideremos la siguiente ecuación: \\[\\begin{equation} Z_t = a_0 + a_1 Z_{t-1} \\tag{2.4} \\end{equation}\\] Donde, \\(t = \\ldots, -2, -1, 0, 1, 2, \\ldots\\), y \\(a_0\\) y \\(a_1 \\neq 0\\) son números reales constantes. De ((2.4) podememos despejar la variable \\(Z_{t-1}\\) y obtener una forma de ecuación en diferencia: \\[\\begin{equation} Z_t - a_1 Z_{t-1} = a_0 \\tag{2.5} \\end{equation}\\] Ahora denotemos a \\(L Z_t = Z_{t-1}\\), es decir, mediante el operador \\(L\\) se puede rezagar una variable dada. En general, podemos decir que el operador tiene dos propiedades, la primera es que es lineal en el sentido de que abre sumas y saca escalares como se muestra en la siguiente expresión para el caso de un (1) rezago: \\[\\begin{equation} L(\\alpha Z_{t} + \\beta) = \\alpha Z_{t-1} + \\beta \\tag{2.6} \\end{equation}\\] Donde \\(\\alpha, \\beta \\in \\mathbb{R}\\) y \\(\\alpha, \\beta \\neq 0\\). Otro reesultado implícito en esta primera propiedad es que el operador rezago aplicado a cualquier escalar dará como resultado el escalar, puesto que este es una constante sin importa el momento \\(t\\) en el cual se encuentre la variable \\(Z\\). La segunda propiedad del operador es que se puede aplicar de forma consecutiva a una misma variable. Es decir, \\(L ( Z_{t-1}) = L L Z_{t} = L^2 Z_{t}\\), por lo que en general tendremos: \\(L^p Z_t = Z_{t-p}\\) (con \\(p \\in \\mathbb{Z}\\)). Así, en el caso de p rezagos la propiedad de linealidad del operador rezago será: \\[\\begin{equation} L^p (\\alpha Z_{t} + \\beta) = \\alpha Z_{t-p} + \\beta \\tag{2.7} \\end{equation}\\] Dicho lo anterio podemos escribir la solución general de (2.5) como: \\[\\begin{eqnarray} Z_t - a_1 L Z_t &amp; = &amp; a_0 \\nonumber \\\\ (1 - a_1 L)Z_t &amp; = &amp; a_0 \\nonumber \\\\ Z_t &amp; = &amp; a_0 \\frac{1}{1 - a_1 L} + s a^t_1 \\nonumber \\\\ Z_t &amp; = &amp; a_0 \\frac{1}{1 - a_1} + s a^t_1 \\tag{2.8} \\end{eqnarray}\\] Donde \\(a_1 \\neq 1\\) y \\(t = \\ldots, -2, -1, 0, 1, 2, \\ldots\\). Notése que la aplicación del operador rezago \\(L\\) a la constante \\(a_1\\) dará como resultado el valor de la misma constante, ya que ésta no depende del momento \\(t\\) en el cuál observemos a la variable \\(Z_t\\). En la ecuación (2.8) se adiciona un término \\(s a^t_1\\) que permite ubicar la trayectoria inicial de la solución de la ecuación. El componente no significa un cambio respecto de la ecuación (2.5) original, ya que si buscaramos reconstruir a ésta ecuación tendríamos: \\[\\begin{eqnarray} (1 - a_1 L) s a^t_1 &amp; = &amp; s a^t_1 - a_1 s L a^{t}_1 \\nonumber \\\\ &amp; = &amp; s a^t_1 - a_1 s a^{t - 1}_1 \\nonumber \\\\ &amp; = &amp; s a^t_1 - s a^t_1 \\nonumber \\\\ &amp; = &amp; 0 \\nonumber \\end{eqnarray}\\] La ecuación (2.8) se suele interpretar como la solución de largo plazo. Ahora demostraremos por qué es cierta la ecuación y discutiremos algunas condiciones que se deben observar en esta solución para que sea una solución convergente. No obstante, primero discutiremos un método indirecto e incompleto para demostrar el resultado, dicho método es conocido como el método iterativo. Plantearemos las siguientes ecuaciones partículares donde suponemos la existencia del valor inicial \\(Z_0\\) del proceso: \\[\\begin{equation*} Z_1 = a_0 + a_1 Z_0 \\end{equation*}\\] \\[\\begin{eqnarray*} Z_2 &amp; = &amp; a_0 + a_1 Z_1 \\\\ &amp; = &amp; a_0 + a_1 (a_0 + a_1 Z_0) \\\\ &amp; = &amp; a_0 + a_0 a_1 + a^2_1 Z_0 \\\\ &amp; = &amp; a_0 (1 + a_1) + a^2_1 Z_0 \\end{eqnarray*}\\] \\[\\begin{eqnarray*} Z_3 &amp; = &amp; a_0 + a_1 Z_2 \\\\ &amp; = &amp; a_0 + a_1 (a_0 + a_0 a_1 + a^2_1 Z_0) \\\\ &amp; = &amp; a_0 + a_0 a_1 + a_0 a^2_1 + a^3_1 Z_0 \\\\ &amp; = &amp; a_0 (1 + a_1 + a^2_1) + a^3_1 Z_0 \\end{eqnarray*}\\] De lo anterior se puede inferir que el método iterativo convergerá hacia una expresión como la siguiente en el momento \\(t\\): \\[\\begin{eqnarray} Z_t &amp; = &amp; a_0 + a_1 Z_{t-1} \\nonumber \\\\ &amp; = &amp; a_0 (1 + a_1 + a^2_1 + \\ldots + a^{t-1}_1) + a^t_1 Z_0 \\nonumber \\\\ &amp; = &amp; a_0 \\sum^{t-1}_{i = 0}{a^i_1} + a^t_1 Z_0 \\tag{2.9} \\end{eqnarray}\\] Donde, es necesario que en la ecuación (2.9) se cumpla que \\(\\lvert{a_1}\\lvert &lt; 1\\) para que la suma sea convergente más adelante detallaremos esta afirmación. A este tipo de ecuaciones se les puede denominar como lineales. Esto en razón de que ningún término de la variable \\(Z\\) aparce elevado a ninguna potencia distinta a 1. También, son de primer orden, ya que el rezago de la variable \\(Z\\) es sólo de un período. En adelante trabajaremos con ecuaciones en las que la variable \\(Z\\) se encuentra rezagada en cualquiera de los siguientes casos: \\[\\begin{equation} Z_t, Z_{t-1}, Z_{t-2}, Z_{t-3}, \\ldots, Z_{t-p}, \\ldots \\tag{2.10} \\end{equation}\\] Por lo que diremos que en adelante el curso versará sobre ecuaciones en diferencia lineales y de cualquier orden \\(p\\). Retomando la ecuación () y considerando la parte de la suma de los términos de \\(a^i_1\\), de tal forma que buscaremos dar una expresión más compresible a dicho término. Definamos la siguiente expresión como: \\[\\begin{equation} S_{t-1} = \\sum^{t-1}_{i = 0}{a^i_1} \\tag{2.11} \\end{equation}\\] Por lo tanto, \\(S_t\\) estaría dado por la siguiente expresión: \\[\\begin{eqnarray} S_{t} &amp; = &amp; a_1 \\sum^{t-1}_{i = 0}{a^i_1} \\nonumber \\\\ &amp; = &amp; a_1 (1 + a_1 + a^2_1 + \\ldots + a^{t-1}_1) \\nonumber \\\\ &amp; = &amp; a_1 + a^2_1 + a^3_1 + \\ldots + a^{t}_1 \\nonumber \\\\ &amp; = &amp; a_1 S_{t-1} \\tag{2.12} \\end{eqnarray}\\] Tomando los dos resultados de las ecuaciones (2.11) y (2.12) anteriores, podemos expresar que si a \\(S_{t-1}\\) le restamos \\(S_t\\), y desarrollando ambos lados de la ecuación anterior podemos obtener: \\[\\begin{eqnarray} S_{t-1} - a_1 S_{t-1} &amp; = &amp; S_{t-1} - S_{t} \\nonumber \\\\ (1 - a_1) S_{t-1} &amp; = &amp; (1 + a_1 + a^2_1 + \\ldots + a^{t-1}_1) - (a_1 + a^2_1 + a^3_1 + \\ldots + a^{t}_1) \\nonumber \\\\ (1 - a_1) S_{t-1} &amp; = &amp; 1 - a^{t}_1 \\nonumber \\end{eqnarray}\\] Así, podemos concluir que: \\[\\begin{equation} S_{t-1} = \\frac{1 - a^{t}_1}{1 - a_1} \\tag{2.13} \\end{equation}\\] Conjuntando éste último resultado de la ecuación (2.13) con la ecuación (2.9) tenemos la siguiente solución por el método de iteración: \\[\\begin{equation} Z_t = a_0 \\left( \\frac{1 - a^{t}_1}{1 - a_1} \\right) + a^t_1 Z_0 \\tag{2.14} \\end{equation}\\] De esta forma la ecuación (2.14) es una solición para la ecuación (2.9), que es una ecuación de un proceso de una Ecuación en Diferencia plantenado en la ecuación (2.4). Está solución aún no es general, en el sentido de que sea válida para cualquiel tipo de proceso: convergente o divergente. Dicha convergencia o divengencia estará determinada por el paramétro \\(a_1\\). No debe pasar desapercibido que cuando \\(t \\rightarrow \\infty\\) o cuando la muestra es muy grande (lo que es equivalente), podemos decir que la solución solo puede converger a la siguiente expresión cuando se considera que \\(|a_1| &lt; 1\\): \\[\\begin{equation} Z_t = a_0 \\left( \\frac{1}{1 - a_1} \\right) \\tag{2.15} \\end{equation}\\] Retomemos ahora el caso general descrito en la ecuación (2.8) y determinemos una solución general en la cual \\(a_1 \\neq 1\\) y \\(t = \\ldots, -2, -1, 0, 1, 2, \\ldots\\). Para ello observemos que el siguiente componente en la ecuación mencionada se puede interpretar como la suma infinita de términos descritos como: \\[\\begin{eqnarray} \\frac{1}{1 - a_1} &amp; = &amp; 1 + a_1 + a_1^2 + \\ldots + a_1^t + \\ldots \\nonumber \\\\ &amp; = &amp; \\sum_{i = 0}^{\\infty} a_1^{i} \\tag{2.16} \\end{eqnarray}\\] Donde claramente es necesario que \\(|a_1| &lt; 1\\). Por lo tanto, sólo faltaría determinar el valor de la constante \\(s\\) en la ecuación () de la siguiente forma, supongamos que observamos el proceso en el momento inicial, por lo que es posible determinar el valor de la constante conociendo el valor inicial del proceso como sigue: \\[\\begin{equation} Z_0 = a_0 \\frac{1}{1 - a_1} + s \\tag{2.17} \\end{equation}\\] De la ecuación (2.17) tenemos que: \\[\\begin{equation} s = Z_0 - a_0 \\frac{1}{1 - a_1} \\tag{2.18} \\end{equation}\\] Así, juntando la ecuación (2.8) y ecuación (2.18) tenemos la expresión: \\[\\begin{equation} Z_t = a_0 \\frac{1 - a^t_1}{1 - a_1} + a^t_1 Z_0 \\tag{2.19} \\end{equation}\\] No debe pasar desapercibido que está solución es la misma que la mostrada en la ecuación ((2.14), por lo que en realidad ambas ecuaciones son una solución general indistintamente entre las ecuaciones (2.14) y @ref(eq:SOL_GEN). Ambas convergen a la expresión como la ecuación (2.15), con la misma condición de convergencia \\(|a_1| &lt; 1\\). Para ilustrar estas ecuaciones veámos algunos ejemplos al respecto. Consideremos que tenemos un proceso \\(Z_t\\) que es descrito por una ecuación en diferencia lineal de primer orden dada por: \\[\\begin{equation} Z_t = 2 + 0.9 Z_{t-1} \\tag{2.20} \\end{equation}\\] Siguiendo la expresión mostrada en la ecuación (2.19), obtenemos la expresión: \\[\\begin{equation} Z_t = 2 \\left( \\frac{1 - 0.9^{t}}{1 - 0.9} \\right) + 0.9^t Z_0 \\tag{2.21} \\end{equation}\\] Donde asumiremos que el valor inicial es \\(Z_0 = 10\\) y que la expresión debe converger al valor de 20, cuando \\(t\\) es muy grande o tiende a infinito. De forma similar tomemos otro ejemplo, en el cual asumimos la siguiente expresión: \\[\\begin{equation} Z_t = 2 - 0.5 Z_{t-1} \\tag{2.22} \\end{equation}\\] Siguiendo la expresión mostrada en la ecuación (), obtenemos: \\[\\begin{equation} Z_t = 2 \\left( \\frac{1 - (-0.5)^{t}}{1 + 0.5} \\right) + (-0.5)^t Z_0 \\tag{2.23} \\end{equation}\\] Donde asumiremos que el valor inicial es \\(Z_0 = 10\\) y que la ecuación converge al valor de \\(1.3333333 \\ldots\\), cuando \\(t\\) es muy grande o tiende a infinito. Ahora simulemos el comportamiento de ambos procesos y estableceremos los resultados del Cuadro . Notemos que el segundo proceso converge de una forma más rapida que el primero. El Cuadro 2.1 se ilustra en las siguientes dos Figura 2.1 y Figura 2.2. Table 2.1: Dos ejemplos de Procesos de Ecuaciones Lineales de Primer Orden Convergentes Tiempo \\(Z_t =2+0.9Z_{t-1}\\) \\(Z_t = 2-0.5Z_{t-1}\\) 0 10.00000 10.000000 1 11.00000 -3.000000 2 11.90000 3.500000 3 12.71000 0.250000 4 13.43900 1.875000 5 14.09510 1.062500 6 14.68559 1.468750 7 15.21703 1.265625 8 15.69533 1.367188 9 16.12580 1.316406 10 16.51322 1.341797 11 16.86189 1.329102 12 17.17570 1.335449 13 17.45813 1.332275 14 17.71232 1.333862 15 17.94109 1.333069 96 19.99960 1.333333 97 19.99964 1.333333 98 19.99967 1.333333 99 19.99970 1.333333 100 19.99973 1.333333 Figure 2.1: Evolución del proceso dado por \\(Z_t =2+0.9Z_{t-1}\\) Figure 2.2: Evolución del proceso dado por \\(Z_t =2-0.5Z_{t-1}\\) 2.1.2 Ecuaciones en Diferencia Lineales de Segundo Orden y de orden superior Como un segundo caso a estudiar se ubica el caso de las Ecuaciones en Diferencia Lineales de Segundo Orden y de orden superior. Primero, sea una ecuación como la siguiente, la cual es lineal y de segundo orden, ya que tiene asociado un término de \\(Z_t\\) rezagado dos periódos: \\[\\begin{equation} Z_t = a_0 + a_1 Z_{t-1} + a_2 Z_{t-2} \\tag{2.24} \\end{equation}\\] Donde \\(t = \\ldots, -2, -1, 0, 1, 2, \\ldots\\) y \\(a_1, a_2 \\neq 0\\). Reordenando la ecuación (2.24) podemos escribir: \\[\\begin{eqnarray} Z_t - a_1 Z_{t-1} - a_2 Z_{t-2} &amp; = &amp; a_0 \\nonumber \\\\ Z_t - a_1 L Z_{t} - a_2 L^2 Z_{t} &amp; = &amp; a_0 \\nonumber \\\\ (1 - a_1 L - a_2 L^2)Z_t &amp; = &amp; a_0 \\tag{2.25} \\end{eqnarray}\\] Así, la solución general propuesta para la ecuación (2.25) es la siguiente, la cual es una forma analóga a una Ecuación Lineal en Diferencia de Primer Orden: \\[\\begin{equation} Z_t = \\frac{a_0}{1 - a_1 - a_2} + s_1 g^t_1 + s_2 g^t_2 \\tag{2.26} \\end{equation}\\] En donde \\(s_1\\) y \\(s_2\\) son constantes que se determinan mediante dos condiciones iniciales por lo que para resolver este tipo de ecuaciones requerimos conocer dos condiciones iniciales. Los valores de \\(g_1\\) y \\(g_2\\) están relacionados con los coeficientes \\(a_1\\) y \\(a_2\\), de esta forma: \\[\\begin{equation} a_1 = g_1 + g_2 \\tag{2.27} \\end{equation}\\] \\[\\begin{equation} a_2 = - g_1 g_2 \\tag{2.28} \\end{equation}\\] Lo anterior surge del siguiente procedimiento y recordando que siempre es posible descomponer una ecuación cuadrática en expresiones como las siguientes: \\[\\begin{eqnarray} (1 - a_1 L - a_2 L^2) &amp; = &amp; (1 - g_1 L)(1 - g_2 L) \\nonumber \\\\ &amp; = &amp; 1 - g_1 L - g_2 L + g_1 g_2 L^2 \\nonumber \\\\ &amp; = &amp; 1 - (g_1 + g_2) L + g_1 g_2 L^2 \\tag{2.29} \\end{eqnarray}\\] Donde se observa la equivalencia mostrada en las ecuaciones (2.27) y (2.28). Así, considerando la ecuación (2.26) tenemos que: \\[\\begin{eqnarray} (1 - a_1 L - a_2 L^2) Z_t &amp; = &amp; (1 - g_1 L)(1 - g_2 L) Z_t \\nonumber \\\\ &amp; = &amp; a_0 + (1 - g_1 L)(1 - g_2 L) s_1 g^t_1 \\nonumber \\\\ &amp; &amp; + (1 - g_1 L)(1 - g_2 L) s_2 g^t_2 \\end{eqnarray}\\] Por lo tanto, buscamos que para que el proceso sea equivalente y podamos interpretar que la ecuación (2.26) sea una solución general deberá pasar lo siguiente: \\[\\begin{equation} (1 - g_1 L) (1 - g_2 L) s_1 g^t_1 + (1 - g_1 L) (1 - g_2 L) s_2 g^t_2 = 0 \\end{equation}\\] O, escrito de otra forma: \\[\\begin{equation} (1 - g_1 L) s_1 g^t_1 = (1 - g_2 L) s_2 g^t_2 = 0 \\end{equation}\\] Ahora determinemos cuáles son los valores \\(g_1\\) y \\(g_2\\) dados los valores \\(a_1\\) y \\(a_2\\) que nos permitan determinar si el proceso será convergente. Para ello debemos resolver la siguiente ecuación que se deriva de la ecuación (2.29): \\[\\begin{equation} 1 - a_1 x - a_2 x^2 = (1 - g_1 x)(1 - g_2 x) = 0 \\end{equation}\\] Donde, claramente existen dos raíces: \\(x_1 = g^{-1}_1\\) y \\(x_2 = g^{-1}_2\\). Así, la solución estará dada por las raíces de la ecuación característica: \\[\\begin{eqnarray} 1 - a_1 x - a_2 x^2 = 0 \\nonumber \\\\ a_2 x^2 + a_1 x - 1 = 0 \\tag{2.30} \\end{eqnarray}\\] Cuya solución es: \\[\\begin{equation} x = \\frac{- a_1 \\pm \\sqrt{a^2_1 + 4 a_2}}{2 a_2} \\end{equation}\\] Es importante distinguir tres diferentes casos en relación con las raíces que surgen como solución de la ecuación (2.30), estos son: Caso I. Si \\(a^2_1 + 4 a_2 &gt; 0\\), la ecuación (2.30) proporcionará dos valores de raíces reales y distintos, eso es \\(x_1 = g^{-1}_1 \\neq x_2 = g^{-1}_2\\). Si por ahora suponemos que \\(|{g_1} &lt; 1|\\) y que \\(|{g_2} &lt; 1|\\), entonces tendremos que: \\[\\begin{eqnarray} (1 - g_1 L)^{-1} (1 - g_2 L)^{-1} a_0 &amp; =&amp; \\left( \\sum^{\\infty}_{j = 0}{g^j_1 L^j} \\right) \\left( \\sum^{\\infty}_{j = 0}{g^j_2 L^j} \\right) a_0 \\nonumber \\\\ &amp; = &amp; \\left( \\sum^{\\infty}_{j = 0}{g^j_1} \\right) \\left( \\sum^{\\infty}_{j = 0}{g^j_2} \\right) a_0 \\nonumber \\\\ &amp; = &amp; \\frac{a_0}{(1 - g_1)(1 - g_2)} \\nonumber \\\\ &amp; = &amp; \\frac{a_0}{1 - a_1 - a_2} \\end{eqnarray}\\] Esto último es el punto de equilibrio de la ecuación (2.26); considerando que \\(|{g_1} &lt; 1|\\) y que \\(|{g_2} &lt; 1|\\) notemos que los demás casos son divergentes, ya que la suma anterior nno connvergería. De esta forma la solución de la ecuación estará dada por: \\[\\begin{equation} \\lim_{t \\to \\infty} Z_t = \\frac{a_0}{1 - a_1 - a_2} \\tag{2.31} \\end{equation}\\] Caso II. Si \\(a_1^2 + 4a_2 &lt; 0\\) en la ecuación (2.30), entonces las raíces serán números complejos conjugados, es decir: \\[\\begin{eqnarray} g_i^{-1} &amp; = &amp; a \\pm ib \\\\ g_i &amp; = &amp; u \\pm iv \\end{eqnarray}\\] Dichas raíces las podemos escribir en coordenadas polares como: \\[\\begin{eqnarray} g_1^{-1} &amp; = &amp; r e^{i \\theta} = r (cos(\\theta) + i sen(\\theta)) \\\\ g_2^{-1} &amp; = &amp; r e^{-i \\theta} = r (cos(\\theta) - i sen(\\theta)) \\end{eqnarray}\\] Donde: \\(r = \\sqrt{u^2 + v^2}\\), a esta expresión también se le conoce como modulo. Alternativamente, podemos escribir que \\(r = \\sqrt{g_1 g_2}\\). La única condición es que \\(r &lt; 1\\) para que el proceso descrito en la ecuación (2.26) sea convergente. Al igual que en el Caso I, el punto de equilibrio de la ecuación se debería ubicar al rededor (2.31), siempre que \\(r &lt; 1\\), por lo que el factor que determina la convergencia es el modulo, ya que si el modulo es mayor a 1, el proceso será divergente, pero si es menor a 1 convergerá a (2.31). Para ilustrar, el caso contrario es divergente puesto que representa trayentorias senoidales (oscilatorias) que sólo pueden converger si a medida que pasa el tiempo, las ondas son menos amplias. Caso III. Ahora revisemos el caso en el que \\(a_1^2 + 4a_2 = 0\\), de esta forma las raíces serán identicas: \\[\\begin{equation} g = g_1^{-1} = g_2^{-1} = \\frac{-a_1}{2 a_2} \\end{equation}\\] Así, el punto de equilibrio será dado por la solución descrita como: \\[\\begin{eqnarray} (1 - g L)^2 Z_t &amp; = &amp; a_0 \\nonumber \\\\ Z_t &amp; = &amp; \\frac{a_0}{(1 - g L)^2} + s_1 g^t + s_2 t g^t \\nonumber \\\\ &amp; = &amp; a_0 \\sum_{i = 0}^{\\infty} (1 + i) g^j + s_1 g^t + s_2 t g^t \\end{eqnarray}\\] Donde la expresión amnterior es resultado de considerar el siguiente procedimiento. Sea: \\[\\begin{eqnarray} f(g) &amp; = &amp; \\frac{1}{(1 - g)} = \\sum_{j = 0}^{\\infty} g^j \\nonumber \\end{eqnarray}\\] Por lo que si hacemos la primer derivada del la expresión anterior tenemos que: \\[\\begin{eqnarray} f&#39;(g) &amp; = &amp; \\frac{1}{(1 - g)^2} \\nonumber \\\\ &amp; = &amp; \\sum_{j = 0}^{\\infty} j g^{j-1} \\nonumber \\\\ &amp; = &amp; 0 + g^0 + 2 g^1 + 3 g^2 + \\ldots \\nonumber \\\\ &amp; = &amp; \\sum_{j = 0}^{\\infty} (1 + j) g^j \\nonumber \\end{eqnarray}\\] Ahora veámos un ejemplo de una Ecuación Lineal en Diferencia de Segundo Orden. Supongamos la ecuación y el desarrollo siguientes: \\[\\begin{eqnarray} Z_t &amp; = &amp; 3 + 0.9 Z_{t-1} - 0.2 Z_{t-2} \\nonumber \\\\ (1 - 0.9 L + 0.2 L^2) Z_t &amp; = &amp; 3 \\nonumber \\end{eqnarray}\\] La solución dada por una ecuación similar a la expresión (2.30), obtendríamos la solución dada por las ecuaciones equivalentes a: \\[\\begin{eqnarray} 1 - 0.9 x + 0.2 x^2 = 0 \\nonumber \\\\ - 0.2 x^2 + 0.9 x - 1 = 0 \\nonumber \\end{eqnarray}\\] De donde las raíces del polinomio característico \\(x_1 = g_1^{-1}\\) y \\(x_2 = g_2^{-1}\\) se obtienen de la expresión dada por: \\[\\begin{eqnarray} x &amp; = &amp;\\frac{-0.9 \\pm \\sqrt{0.81 + (4)(-0.2)}}{(2)(-0.2)} \\nonumber \\\\ &amp; = &amp; \\frac{0.9 \\pm 0.1}{0.4} \\nonumber \\end{eqnarray}\\] Dado que el componente \\(a^2_1 + 4 a_2\\) es positivo, obtendremos dos raíces reales. Las raíces estarán dadas por \\(x_1 = 2.5\\) y \\(x_2 = 2.0\\), de lo cual podemos determinar que \\(g_1 = 0.4\\) y \\(g_2 = 0.5\\). De esta forma tenemos que \\(|g_1| &lt; 1\\) y \\(|g_2| &lt; 1\\), así la ecuación converge a la expresión dada por las siguientes expresiones: \\[\\begin{eqnarray} Z_t &amp; = &amp; \\frac{3}{1 - 0.9 L + 0.2 L^2} + s_1 (0.4)^t + s_2 (0.5)^t \\nonumber \\\\ &amp; = &amp; \\frac{3}{1 - 0.9 + 0.2} + s_1 (0.4)^t + s_2 (0.5)^t \\nonumber \\\\ &amp; = &amp; \\frac{3}{(1 - 0.4)(1 - 0.5)} + s_1 (0.4)^t + s_2 (0.5)^t \\nonumber \\end{eqnarray}\\] Al final, la ecuación que describe la solución general será: \\[\\begin{equation} z_t = 10 + s_1 (0.4)^t + s_2 (0.5)^t \\end{equation}\\] Para determinar los valores de \\(s_1\\) y \\(s_2\\) necesitamos obtener dos valores iniciales de la ecuación para lo cual iniciaremos como \\(t = 0\\) y luego obtenemos el valor de \\(t = 1\\), consideremos el valor de \\(Z_0 = 0\\) y \\(Z_1 = 50\\): \\[\\begin{eqnarray*} Z_0 &amp; = &amp; 10 + s_1(0.4)^0 + s_2(0.5)^0 \\\\ 0 &amp; = &amp; 10 + s_1 + s_2 \\\\ Z_1 &amp; = &amp; 10 + s_1(0.4)^1 + s_2(0.5)^1 \\\\ 50 &amp; = &amp; 10 + 0.4 s_1 + 0.5 s_2 \\end{eqnarray*}\\] Por lo que la solución es: \\(s_1 = -450\\) y \\(s_2 = 440\\), de donde podemos expresar la ecuación como: \\[\\begin{equation} Z_t = 10 - 450(0.4)^t + 440(0.5)^t \\tag{2.32} \\end{equation}\\] La ecuación (2.32) anterior convergerá al valor de 10 cuando \\(t \\rightarrow \\infty\\). Para ilustrar la trayectoria de esta ecuación tomemos un cuadro similar al de los ejemplos anteriores. En el Cuadro 2.2 y la Figura 2.3 mostramos los resultados de la trayectorua para 100 periodos. Table 2.2: Un ejemplo de proceso de Ecuación de Segundo Orden convengente Tiempo \\(Z_t =10-450(0.4)^t+440(0.5)^t\\) 0 0.00000 1 50.00000 2 48.00000 3 36.20000 4 25.98000 5 19.14200 6 15.03180 7 12.70022 8 11.42384 9 10.74141 10 10.38250 11 10.19597 12 10.09987 13 10.05069 14 10.02565 15 10.01294 96 10.00000 97 10.00000 98 10.00000 99 10.00000 100 10.00000 Figure 2.3: Evolución del proceso dado por \\(Z_t =3+0.9Z_{t-1}-0.2Z_{t-2}\\) Finalmente, discutiremos la solución para las Ecuaciones Lineales en Diferencia de Orden \\(p\\), donde \\(p \\geq 2\\). En general una ecuación de este tipo se puede escribir como: \\[\\begin{equation} Z_t = a_0 + a_1 Z_{t-1} + a_2 Z_{t-2} + \\ldots + a_p Z_{t-p} \\tag{2.33} \\end{equation}\\] Donde \\(t = \\ldots, -2, -1, 0, 1, 2, \\ldots\\) y \\(a_p \\neq 0\\). La ecuación (2.33) se puede escribir como: \\[\\begin{eqnarray} Z_t - a_1 Z_{t-1} - a_2 Z_{t-2} - \\ldots - a_p Z_{t-p} &amp; = &amp; a_0 \\nonumber \\\\ Z_t - a_1 L Z_t - a_2 L^2 Z_t - \\ldots - a_p L^p Z_t &amp; = &amp; a_0 \\nonumber \\\\ (1 - a_1 L - a_2 L^2 - \\ldots - a_p L^p) Z_t &amp; = &amp; a_0 \\tag{2.34} \\end{eqnarray}\\] Por el Teorema Fundamental del Álgebra es posible escribir a la ecuación (2.34) como: \\[\\begin{eqnarray} (1 - g_1 L)(1 - g_1 L) \\ldots (1 - g_p L) Z_t &amp; = &amp; a_0 \\tag{2.35} \\end{eqnarray}\\] Utilizando la ecuación (2.34) y la ecuación (2.35) tenemos que la solución general de una ecuación como la descrita en (2.33) se puede escribir como: \\[\\begin{equation} Z_t = \\frac{a_0}{1 - a_1 - a_2 - \\ldots - a_p} + s_1 g^t_1 + s_2 g^t_2 + \\ldots + s_p g^t_p \\\\ \\tag{2.36} \\end{equation}\\] \\[\\begin{eqnarray} Z_t &amp; = &amp; \\frac{a_0}{(1 - g_1)(1 - g_1) \\ldots (1 - g_p)} + s_1 g^t_1 + s_2 g^t_2 + \\ldots + s_p g^t_p \\tag{2.37} \\end{eqnarray}\\] Donde \\(s_1\\), \\(s_2\\), , \\(s_p\\) son cosntantes que se determinan utilizando \\(p\\) valores partículares de \\(Z_t\\), y la solución general descrita en las ecuaciones (2.36) y (2.37) implica encontrar \\(p\\) raíces: \\(x_1 = g^{-1}_1\\), \\(x_2 = g^{-1}_2\\), , \\(x_p = g^{-1}_p\\) de los siguientes polinomios equivalentes: \\[\\begin{eqnarray} (1 - g_1)(1 - g_1) \\ldots (1 - g_p) = 0 \\\\ 1 - a_1 x - a_2 x^2 - \\ldots - a_p x^p = 0 \\\\ \\tag{2.38} a_p x^p + \\ldots + a_2 x^2 + a_1 x - 1 = 0 \\end{eqnarray}\\] Antes de plantear la solución general, analicemos una solución patícular cuando un conjunto de las \\(p\\) raíces, digamos un total de \\(m\\), son iguales, es decir, cuando sucede que \\(g_1 = g_2 = \\ldots = g_m = g\\) (con \\(1 &lt; m \\leq p\\)). En este caso la solución general en la ecuación (2.37) se escribe como: \\[\\begin{eqnarray} Z_t &amp; = &amp; \\frac{a_0}{(1 - g)^m(1 - g_{m+1}) \\ldots (1 - g_p)} \\nonumber \\\\ &amp; &amp; + s_1 g^t + s_2 t g^t + \\ldots + s_m t^{m-1} g^t + s_{m+1} g^t_{m+1} + \\ldots + s_{p} g^t_{p} \\tag{2.39} \\end{eqnarray}\\] Definamos: \\[\\begin{equation} f(g) = \\frac{1}{1 - g} = \\sum_{j = 0}^{\\infty} g^j \\end{equation}\\] Si retomamos el método descrito parráfos arriba tenemos las siguientes expresiones. Cuando \\(m = 2\\): \\[\\begin{equation} f&#39;(g) = \\frac{1}{(1 - g)^2} = \\sum_{j = 0}^{\\infty} j g^{j-1} = \\sum_{j = 0}^{\\infty} (1 + j) g^j \\nonumber \\end{equation}\\] En el otro extremo, cuando \\(m = p\\): \\[\\begin{equation} f^{(p-1)}(g) = \\frac{p-1}{(1 - g)^p} = \\sum_{j = 0}^{\\infty} \\frac{(p-1+j)(p-2+j) \\ldots (2+j)(1+j)}{(p-1)!} g^j \\end{equation}\\] Así, en el extremo cuando \\(m = p\\) la solución general podría estar dada por: \\[\\begin{eqnarray} Z_t &amp; = &amp; a_0 \\sum_{j = 0}^{\\infty} \\frac{(p-1+j)(p-2+j) \\ldots (2+j)(1+j)}{(p-1)!} g^j \\nonumber &amp; &amp; + g^t \\sum_{i = 0}^p s_i t^{i-1} \\tag{2.40} \\end{eqnarray}\\] Donde \\(|{g} &lt; 1|\\), \\(t = \\ldots, -2, -1, 0, 1, 2, \\ldots\\). Para finalizar esta sección, plantearemos la expresión de polinomio característico que nos permitirá hacer el análisis de convergencia de los procesos. Partamos de que la ecuación (2.38) se puede escribir como: \\[\\begin{equation} (x^{-1})^p - a_1 (x^{-1})^{p-1} - a_2 (x^{-1})^{p-1} - \\ldots - a_p = 0 \\tag{2.41} \\end{equation}\\] La ecuación (2.41) permite interpretar las raíces del polinomio característico de forma directa ya que \\(x^{-1}_1 = g_1\\), \\(x^{-1}_2 = g_2\\), , \\(x^{-1}_p = g_p\\). Así, siempre que \\(p \\geq 1\\) en la ecuación (2.33), diremos que el proceso descrito en esa ecuación dará como resultado un proceso convergente si se cumplen las dos condiciones (2.42) y (2.43): \\[\\begin{equation} |a_p| &lt; 1 \\tag{2.42} \\end{equation}\\] \\[\\begin{equation} a_1 + a_2 + \\ldots + a_p &lt; 1 \\tag{2.43} \\end{equation}\\] Alternativamente, cuando las raíces son reales lo anterior es equivalente a la expresión (2.44): \\[\\begin{eqnarray} |g_i| &lt; 1 \\tag{2.44} \\end{eqnarray}\\] Para \\(\\forall i = 1, 2, \\ldots, p\\). Cuando la raíces son imaginarias, las dos condiciones (2.42) y (2.43) son equivalentes a la expresión (2.45): \\[\\begin{eqnarray} \\sqrt{g_i g_j} = \\sqrt{u^2 + v^2} &lt; 1 \\tag{2.45} \\end{eqnarray}\\] Para \\(\\forall i \\neq j\\) y \\(i, j = 1, 2, \\ldots, p\\). Cuando \\(g_1 = g_2 = \\ldots = g_p = g\\), la condición de la ecuación (2.44) se resume a que \\(|g| &lt; 1\\). En resumen, las condiciones descritas en las ecuaciones (2.44) y (2.45) se puden ilustrar con un circulo unitario como el de la Figura 2.4 en que sí las raíces se ubican dentro de éste, podemos decir que el proceso es convergente en el largo plazo. Figure 2.4: Circulo unitario en el que se cumple que \\(|g_i|&lt;1\\) y \\((g_i g_j)^{1/2} = (u^2 + v^2)^{1/2} &lt; 1\\) 2.2 Operador de rezago L Denotemos, como se ha mencionado con anterioridad, con \\(L\\) al operador de rezago, el cual nos permitirá construir una relación entre diferencias y medias móviles como se verá más adelante en los procesos univariados \\(AR(p)\\), \\(MA(q)\\) y, en general, \\(ARIMA(p, d, q)\\). Sean \\(X\\), \\(Y\\) o \\(Z\\) variables con las que denotaremos a una serie de tiempo (note que hasta el momento no hemos definido qué es una serie de tiempo, no obstante no es necesario definirla para hacer uso del operador). En esta sección resumiremos algunas propiedades usadas en el capítulo y en capítulos más adelante. Así, si a dicha serie le aplicamos el operador rezago antes definido, el resultado deberá ser que cada uno de los valores de la serie es retardado o regresado un período. Es decir: \\[\\begin{equation} L Z_t = Z_{t-1} \\tag{2.46} \\end{equation}\\] De esta forma, si aplicamos el operador rezago \\(L\\) a la nueva serie de tiempo dada por \\(Z_{t-1}\\) podemos obtener \\(Z_{t-2}\\), haciendo uso de la ecuación (2.46) podemos obtener: \\[\\begin{equation} L Z_{t-1} = L(L Z_t) = L^2 Z_t = Z_{t-2} \\end{equation}\\] Mediante una generalización podemos obtener: \\[\\begin{equation} L^k Z_t = Z_{t-k} \\end{equation}\\] Para \\(k = \\ldots, -2, -1, 0, 1, 2, \\ldots\\). Así, para \\(k = 0\\) obtenemos la identidad dado que \\(L^0 Z_t = Z_t\\), de tal forma que siempre asumiremos que \\(L^0 = 1\\). En otro caso, cuando \\(k &gt; 0\\) a la serie de tiempo a la cual se le aplique el operador rezago \\(L\\) se le deberá aplicar un rezago de \\(k\\) periodos a cada uno de los elementos de la serie. Por el contrario, cuando \\(k &lt; 0\\) el operador rezago significa que se deberá adelantar \\(|k|\\) veces a cada elemento de la serie. Por ejemplo, \\(L^{-3} Z_t = Z_{t+3}\\). Las reglas descritas en lo subsecuente se mantienen indistintamene cuando aplican para el caso de rezagar como para cuando se adelanta una serie. Como primera propiedad tomemos a la siguiente propiedad: \\[\\begin{equation} L^{m} Z_{t-n} = L^{m} (L^{n} Z_{t}) = L^{m + n} Z_{t} = Z_{t-(n + m)} \\end{equation}\\] De lo anterior podemos inferir el siguiente resultado: \\[\\begin{equation} \\Delta Z_{t} = Z_{t} - Z_{t-1} = (1 - L) Z_{t} \\end{equation}\\] En el caso de la diferencia de órden cuatro o cuarta diferencia se puede expresar como: \\[\\begin{equation} \\Delta_{4} Z_{t} = Z_{t} - Z_{t-4} = (1 - L^4) Z_{t} \\tag{2.47} \\end{equation}\\] Al respecto, vale la pena aclarar que en ocaciones se hará uso de una notación alternativa dada por: \\(\\Delta^k\\) o \\(\\Delta_k\\), donde \\(k = 1, 2, 3, \\ldots\\), indistintamente, ya que en ambos casos se referirá a una diferencia de orden \\(k\\). Esta notación resulta de gran utilidad cuando se quiere comparar periodos equivalentes como, por ejemplo, el mismo trimestre pero de un año anterior. De forma similar, para el caso de logarítmos podemos escribir a la ecuación (2.47) como: \\[\\begin{equation} \\Delta^{4} ln(Z_{t}) = \\Delta_{4} ln(Z_{t}) = ln(Z_{t}) - ln(Z_{t-4}) = (1 - L^4) ln(Z_{t}) \\end{equation}\\] Para el caso de una serie de tiempo que se le ha transformado mediante medias móviles, digamos de \\(4\\) periodos, podemos escribirla como: \\[\\begin{equation} Zs_{t} = \\frac{1}{4}(Z_{t} + Z_{t-1} + Z_{t-2} + Z_{t-3}) = \\frac{1}{4}(1 + L + L^2 + L^3)Z_{t} \\end{equation}\\] Una generalización del anterior caso puede ser escrito como un polinomio de orden \\(p\\) con el operador rezago \\(L\\) dado como: \\[\\begin{eqnarray} \\alpha(L) Z_{t} &amp; = &amp; (1 - \\alpha_1 L - \\alpha_2 L^2 - \\ldots - \\alpha_p L^p) Z_{t} \\nonumber \\\\ &amp; = &amp; Z_{t} - \\alpha_1 Z_{t-1} - \\alpha_2 Z_{t-2} - \\ldots - \\alpha_p Z_{t-p} (\\#ec:Ecp1) \\end{eqnarray}\\] Donde \\(\\alpha_i\\) puede ser remplazada por cualquier constante \\(a_i\\), con \\(i = 1, 2, 3, \\ldots\\), para escribir ecuaciones como las anteriores. Adicionalmente, podemos decir que la ecuación (??) es una generalización del caso de medias móviles, el cual admite una poderación distinta para cada uno de los elementos rezagados. Existe la posibilidad de operar más de un polinomio a la vez. Para múltiples polinomios (digamos, los polinomios \\(\\alpha(L)\\) y \\(\\beta(L)\\)) podemos escribir el siguiente resultado: \\[\\begin{equation} \\alpha(L) \\beta(L) = \\beta(L) \\alpha(L) \\end{equation}\\] Tales polinomios del operador rezago también son llamados filtros lineales. A manera de ejemplo tomemos el siguiente caso de diferencias para una serie de \\(Z_t\\): \\[\\begin{equation} \\Delta Z_{t} = (1 - L) Z_{t} = Z_{t} - Z_{t-1} \\end{equation}\\] y un proceso de medias móviles para la misma serie de \\(Z_t\\): \\[\\begin{equation} Zs_{t} = \\frac{1}{4}(1 + L^1 + L^2 + L^3) Z_{t} = \\frac{1}{4}(Z_{t} + Z_{t-1} + Z_{t-2} + Z_{t-3}) \\end{equation}\\] De tal forma que el producto de ambos procesos se puede escribir como: \\[\\begin{equation} (1 - L) \\times \\frac{1}{4}(1 + L^1 + L^2 + L^3) Z_{t} = \\frac{1}{4}(1 - L^4) Z_{t} \\end{equation}\\] Es decir, que el producto de dos polinomios, uno de diferencias y otro más de medias móviles, resulta en uno de diferencias pero de mayor grado, en este caso de grado 4. "],["modelos-de-series-de-tiempo-estacionarias.html", "Capítulo 3 Modelos de Series de Tiempo Estacionarias 3.1 Definición de ergodicidad y estacionariedad 3.2 Función de autocorrelación", " Capítulo 3 Modelos de Series de Tiempo Estacionarias 3.1 Definición de ergodicidad y estacionariedad A partir de esta sección introduciremos mayor formalidad matemática al análisis, por ello cambiaremos de notación y ocuparemos a \\(X_t\\) en lugar de \\(Z_t\\). Con \\(X_t\\) denotaremos a una serie de tiempo, ya que con \\(Z_t\\) denotareemos a una variable, sin que ella fuera necesariamente una serie de tiempo. Asimismo, iniciaremos por establecer una serie de definiciones. De esta forma, definiremos a una serie de tiempo como un vector de variables aleatorias de dimensión \\(T\\), dado como: \\[\\begin{equation} X_1, X_2, X_3, \\ldots ,X_T \\end{equation}\\] Cada una de las \\(X_t\\) (\\(t = 1, 2, \\ldots, T\\)) consideradas como una variable aleatoria. Así, también podemos denotar a la serie de tiempo como: \\[\\begin{equation} \\{ X_t \\}^T_{t = 1} \\tag{3.1} \\end{equation}\\] Es decir, definiremos a una serie de tiempo como una realización de un proceso estocástico o un Proceso Generador de Datos (PGD). Consideremos una muestra de los múlples posibles resultados de muestras de tamaño \\(T\\), la colección dada por: \\[\\begin{equation} \\{X^{(1)}_1, X^{(1)}_2, \\ldots, X^{(1)}_T\\} \\end{equation}\\] es una de las tantas posibles resultantes del proceso estocástico o PGD. Eventualmente podríamos estar dispuestos a observar este proceso indefinidamente, de forma tal que estemos interesados en observar a la secuencia dada por \\(\\{ X^{(1)}_t \\}^{\\infty}_{t = 1}\\), lo cual no dejaría se ser sólo una de las tantas realizaciones o secuencias del proceso estocástico original. Tan solo para poner un ejemplo, podríamos observar las siguientes realizaciones del mismo PGD: \\[\\begin{eqnarray*} &amp; \\{X^{(2)}_1, X^{(2)}_2, \\ldots, X^{(2)}_T\\} &amp; \\\\ &amp; \\{X^{(3)}_1, X^{(3)}_2, \\ldots, X^{(3)}_T\\} &amp; \\\\ &amp; \\{X^{(4)}_1, X^{(4)}_2, \\ldots, X^{(4)}_T\\} &amp; \\\\ &amp; \\vdots &amp; \\\\ &amp; \\{X^{(j)}_1, X^{(j)}_2, \\ldots, X^{(j)}_T\\} &amp; \\end{eqnarray*}\\] Donde \\(j \\in \\mathbb{Z}\\). En lo subsecuente, diremos que una serie de tiempo es una realización del proceso estocástico subyacente. Considerando, en consecuencia, al proceso estocástico con todas sus posibilidades de realización. Para hacer más sencilla la notación no distinguiremos entre el proceso en sí mismo y una de sus realizaciones, es decir, siempre escribiremos a una serie de tiempo como la secuencia mostrada en la ecuación (3.1), o más precisamente como la siguiente realización: \\[\\begin{equation} \\{ X_1, X_2, \\ldots, X_T \\} \\end{equation}\\] O simplemente: \\[\\begin{equation} X_1, X_2, \\ldots, X_T \\end{equation}\\] El proceso estocástico de dimensión \\(T\\) puede ser completamente descrito por su función de distribución multivaraida de dimensión \\(T\\). No obstante, esto no resulta ser práctico cuando se opere más adelante en el curso. Por ello, en el curso, y en general casi todos los textos lo hacen, sólo nos enfocaremos en sus primer y segundo momentos, es decir, en sus medias o valores esperados: \\[\\begin{equation*} \\mathbb{E}[X_t] \\end{equation*}\\] Para \\(t = 1, 2, \\ldots, T\\); o: \\[\\begin{equation*} \\left[ \\begin{array}{c} \\mathbb{E}[X_1] \\\\ \\mathbb{E}[X_2] \\\\ \\vdots \\\\ \\mathbb{E}[X_T] \\end{array} \\right] \\end{equation*}\\] o, \\[\\begin{equation*} \\left[ \\begin{array}{c} \\mathbb{E}[X_1], \\mathbb{E}[X_2], \\ldots, \\mathbb{E}[X_T] \\end{array} \\right] \\end{equation*}\\] De sus variazas: \\[\\begin{equation*} Var[X_t] = \\mathbb{E}[(X_t - \\mathbb{E}[X_t])^2] \\end{equation*}\\] Para \\(t = 1, 2, \\ldots, T\\), y de sus \\(T(T-1)/2\\) covarianzas: \\[\\begin{equation*} Cov[X_t,X_s] = \\mathbb{E}[(X_t - \\mathbb{E}[X_t])(X_s - \\mathbb{E}[X_s])] \\end{equation*}\\] Para \\(t &lt; s\\). Por lo tanto, en la forma matricial podemos escribir lo siguiente: \\[\\begin{equation*} \\left[ \\begin{array}{c c c c} Var[X_1] &amp; Cov[X_1,X_2] &amp; \\cdots &amp; Cov[X_1,X_T] \\\\ Cov[X_2,X_1] &amp; Var[X_2] &amp; \\cdots &amp; Cov[X_2,X_T] \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ Cov[X_T,X_1] &amp; Cov[X_T,X_2] &amp; \\cdots &amp; Var[X_T] \\\\ \\end{array} \\right] \\end{equation*}\\] \\[\\begin{equation} = \\left[ \\begin{array}{c c c c} \\sigma_1^2 &amp; \\rho_{12} &amp; \\cdots &amp; \\rho_{1T} \\\\ \\rho_{21} &amp; \\sigma_2^2 &amp; \\cdots &amp; \\rho_{2T} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\rho_{T1} &amp; \\rho_{T2} &amp; \\cdots &amp; \\sigma_T^2 \\\\ \\end{array} \\right] \\tag{3.2} \\end{equation}\\] Donde es claro que en la matriz de la ecuación (3.2) existen \\(T(T-1)/2\\) covarianzas distintas, ya que se cumple que \\(Cov[X_t,X_s] = Cov[X_s,X_t]\\), para \\(t \\neq s\\). A menudo, esas covarianzas son denominadas como autocovarianzas puesto que ellas son covarianzas entre variables aleatorias pertenecientes al mismo proceso estocástico pero en un momento \\(t\\) diferente. Si el proceso estocástico tiene una distribución normal multivariada, su función de distribución estará totalmente descrita por sus momentos de primer y segundo orden. Ahora introduciremos el concepto de ergodicidad, el cual indica que los momentos muestrales, los cuales son calculados en la base de una serie de tiempo con un número finito de observaciones, en la medida que \\(T \\rightarrow \\infty\\) sus correspondientes momentos muestrales, tienden a los verdaderos valores poblacionales, los cuales definiremos como \\(\\mu\\), para la media, y \\(\\sigma^2_X\\) para la varianza. Este concepto sólo es cierto si asumimos que, por ejemplo, el valor esperado y la varianza son como se dice a continuación para todo \\(t = 1, 2, \\ldots, T\\): \\[\\begin{eqnarray} \\mathbb{E}[X_t] = \\mu_t = \\mu \\\\ \\label{MEDIA} Var[X_t] = \\sigma^2_X \\tag{3.3} \\end{eqnarray}\\] Mas formalmente, se dice que el PGD o el proceso estocástico es ergódico en la media si: \\[\\begin{equation} \\displaystyle\\lim_{T \\to \\infty}{\\mathbb{E} \\left[ \\left( \\frac{1}{T} \\sum^{T}_{t = 1} (X_t - \\mu) \\right) ^2 \\right]} = 0 \\end{equation}\\] y ergódico en la varianza si: \\[\\begin{equation} \\displaystyle\\lim_{T \\to \\infty}{\\mathbb{E} \\left[ \\left( \\frac{1}{T} \\sum^{T}_{t = 1} (X_t - \\mu) ^2 - \\sigma^2_X \\right) ^2 \\right]} = 0 \\end{equation}\\] Estas condiciones se les conoce como propiedades de consistencia para las variables aleatorias. Sin embargo, éstas no pueden ser probadas. Por ello se les denomina como un supuesto que pueden cumplir algunas de las series. Más importante aún: un proceso estocástico que tiende a estar en equilibrio estadístico en un orden ergódico, es estacionario. Podemos distinguir dos tipos de estacionariedad. Si asumimos que la función común de distribución del proceso estocástico no cambia a lo largo del tiempo, se dice que el proceso es estrictamente estacionario. Como este concepto es dificil de aplicar en la práctica, solo consideraremos a la estacionariedad débil o estacionariedad en sus momentos. Definiremos a la estacionariedad por sus momentos del correspondiente proceso estocástico dado por \\(\\{X_t\\}\\): Estacionariedad en media: Un proceso estocástico es estacionario en media si \\(E[X_t] = \\mu_t = \\mu\\) es constante para todo \\(t\\). Estacionariedad en varianza: Un proceso estocástico es estacionario en varianza si \\(Var[X_t] = \\mathbb{E}[(X_t - \\mu_t)^2] = \\sigma^2_X = \\gamma(0)\\) es constante y finita para todo \\(t\\). Estacionariedad en covarianza: Un proceso estocástico es estacionario en covarianza si \\(Cov[X_t,X_s] = \\mathbb{E}[(X_t - \\mu_t)(X_s - \\mu_s)] = \\gamma(|s-t|)\\) es sólo una función del tiempo y de la distancia entre las dos variables aleatorias. Por lo que no depende del tiempo denotado por \\(t\\) (no depende de la información contemporánea). Estacionariedad débil: Como la estacionariedad en varianza resulta de forma inmediata de la estacionariedad en covarianza cuando se asume que \\(s = t\\), un proceso estocástico es débilmente estacionario cuando es estacionario en media y covarianza. \\end{enumerate} Puesto que resulta poco factible asumir una estacionariedad diferente a la débil, es adelante siempre que digamos que un proceso es estacionario se referirá al caso débil y sólo diremos que el proceso es estacionario, sin el apelativo de débil. Ahora veamos un ejemplo de lo anterior. Supongamos una serie de tiempo denotada por: \\(\\{U_t\\}^T_{t = 0}\\). Decimos que el proceso estocástico \\(\\{U_t\\}\\) es un o es un , si éste tiene las siguientes propiedades: En palabras. Un proceso \\(U_t\\) es un ruido blanco si su valor promedio es cero (0), tiene una varianza finita y constante, y además no le importa la historia pasada, así su valor presente no se ve influenciado por sus valores pasados no importando respecto de que periodo se tome referencia. En apariencia, por sus propiedades, este proceso es débilmente estacionario o simplemente, estacionario. Todas las variables aleatorias tienen una media de cero, una varianza \\(\\sigma^2\\) y no existe correlación entre ellas. Ahora supongamos que definimos un nuevo proceso estocástico \\(\\{X_t\\}\\) como: \\[\\begin{equation} X_t = \\left\\{ \\begin{array}{l} U_0 \\mbox{ para } t = 0 \\\\ X_{t-1} + U_t \\mbox{ para } t = 1, 2, 3, \\ldots \\end{array}\\right. \\end{equation}\\] Donde \\(\\{ U_t \\}\\) es un proceso puramente aleatorio. Este proceso estocástico, o caminata aleatoria sin tendencia (ajuste - drift), puede ser reescrito como: \\[\\begin{equation} X_t = \\sum^t_{j = 0} U_j \\end{equation}\\] Tratemos de dar más claridad al ejemplo, para ello asumamos que generamos a \\(\\{U_t\\}\\) por medio del lanzamiento de una moneda. Donde obtenemos una cara con una probabilidad de \\(0.5\\), en cuyo caso decimos que la variable aleatoria \\(U_t\\) tomará el valor de \\(+1\\), y una cruz con una probabilidad de \\(0.5\\), en cuyo caso decimos que la variable aleatoria \\(U_t\\) toma el valor de \\(-1\\). Este planteamiento cumple con las propiedas enunciadas ya que: Retomando a nuestro proceso \\(X_t\\), diremos que el caso de \\(X_0 = 0\\), para \\(t = 0\\). Si verificamos cúales son sus primeros y segundos momentos de \\(\\{X_t\\}\\) tenemos: \\[\\begin{equation} \\mathbb{E}[X_t] = \\mathbb{E}\\left[ \\sum^t_{j=1} U_j \\right] = \\sum^t_{j=1} \\mathbb{E}[U_j] = 0 \\end{equation}\\] En cuanto a la varianza: \\[\\begin{eqnarray} Var[X_t] &amp; = &amp; Var \\left[ \\sum^t_{j=1} U_j \\right] \\nonumber \\\\ &amp; = &amp; \\sum^t_{j=1} Var[U_j] + 2 * \\sum_{j \\neq k} Cov[U_j,U_k] \\nonumber \\\\ &amp; = &amp; \\sum^t_{j=1} 1 \\nonumber \\\\ &amp; = &amp; t \\end{eqnarray}\\] Lo anterior, dado que hemos supuesto que en la caminata aleatoria todas la variables aleatorias son independientes, es decir, \\(Cov[U_t,U_s] = E[U_t \\cdot U_s] = 0\\). Por su parte, la covarianza del proceso estocástico se puede ver como: \\[\\begin{eqnarray*} Cov[X_t,X_s] &amp; = &amp; \\mathbb{E} \\left[ \\left( \\sum^t_{j=1} U_j - 0 \\right) \\left( \\sum^s_{i=1} U_i - 0 \\right) \\right] \\\\ &amp; = &amp; \\mathbb{E}[(U_1 + U_2 + \\ldots + U_t)(U_1 + U_2 + \\ldots + U_s)] \\\\ &amp; = &amp; \\sum^t_{j=1} \\sum^s_{i=1} \\mathbb{E}[U_j U_i] \\\\ &amp; = &amp; \\mathbb{E}[U^2_1] + \\mathbb{E}[U^2_2] + \\ldots + \\mathbb{E}[U^2_k] \\\\ &amp; = &amp; \\sigma^2 + \\sigma^2 + \\ldots + \\sigma^2 \\\\ &amp; = &amp; 1 + 1 + 1 + 1 \\\\ &amp; = &amp; min(t,s) \\end{eqnarray*}\\] Así, el proceso estocástico dado por la caminata alaeatoria sin un término de ajuste es estacionario en media, pero no en varianza o en covarianza, y consecuentemente, en general no estacionario, condición que contraria al caso del proceso simple descrito en \\(U_t\\). Es facil ver que muchas de las posibilidades de realización de este proceso estocástico (series de tiempo) pueden tomar cualquiera de las rutas consideradas en el Figura 3.1. Figure 3.1: Ejemplo de 10 trayectorias de la caminata aleatoria, cuando sólo es posible cambios de +1 y -1$ 3.2 Función de autocorrelación Para ampliar la discusión, es posible calcular la fuerza o intensidad de la dependencia de las variables aleatorias dentro de un proceso estocástico, ello mediante el uso de las autocovarianzas. Cuando las covarianzas son normalizadas respecto de la varianza, el resultado es un término que es independiente de las unidad de medida aplicada, y se conoce como la . Para procesos estacionarios, dicha función de autocorrelación esta dada por: \\[\\begin{equation} \\rho(\\tau) = \\frac{\\mathbb{E}[(X_t - \\mu)(X_{t+\\tau} - \\mu)]}{\\mathbb{E}[(X_t - \\mu)^2]} = \\frac{\\gamma(\\tau)}{\\gamma(0)} \\end{equation}\\] Donde \\(\\tau = \\ldots, -2, -1, 0, 1, 2, \\ldots\\). Dicha función tiene las siguientes propiedades: Derivado de las propiedades 1 y 2 antes descritas se puede concluir que sólo es necesario conocer la función de autocorrelación para el caso de \\(\\tau = 1, 2, 3, \\ldots\\), ya que de estos casos podemos derivar los valores de la función de autocorrelación complementarios de \\(\\tau = \\ldots, -3, -2, -1\\). Partiendo de los supuestos de ergodicidad en relación a la media, varianza y covarianzas de un proceso estacionario, podemos estimar dichos paramétros con las siguientes formulaciones o propuestas de estimadores puntuales: \\[\\begin{equation} \\hat{\\mu} = \\frac{1}{T} \\sum^T_{t=1} X_t \\end{equation}\\] \\[\\begin{equation} \\hat{\\gamma}(0) = \\frac{1}{T} \\sum^T_{t=1} (X_t - \\hat{\\mu})^2 = \\hat{\\sigma}^2 \\end{equation}\\] \\[\\begin{equation} \\hat{\\gamma}(\\tau) = \\frac{1}{T} \\sum^{T - \\tau}_{t=1} (X_t - \\hat{\\mu})(X_{t+\\tau} - \\hat{\\mu}) \\mbox{, para } \\tau = 1, 2, \\ldots, T-1 \\end{equation}\\] No hacemos la demostración en estas notas sería deseable que el alumno revisará la afimación pero estos últimos son estimadores consistentes de \\(\\mu\\), \\(\\gamma(0)\\) y \\(\\gamma(\\tau)\\). Por su parte, un estimador consistente de la función de autocorrelación estará dado por: \\[\\begin{equation} \\hat{\\rho}(\\tau) = \\frac{\\sum^{T - \\tau}_{t=1} (X_t - \\hat{\\mu})(X_{t+\\tau} - \\hat{\\mu})}{\\sum^T_{t=1} (X_t - \\hat{\\mu})^2} = \\frac{\\hat{\\gamma}(\\tau)}{\\hat{\\gamma}(0)} \\mbox{, para } \\tau = 1, 2, \\ldots, T-1 \\label{Eq_AutoCorr} \\end{equation}\\] El estimador de la ecuación () es asintóticamente insesgado. Por ejemplo, para el caso de un proceso de ruido blanco o caminata aleatoria, su varianza puede ser aproximada por el valor dado \\(1/T\\). Ésta tiene, asintóticamente, una distribución normal. Dado esto, el intervalo de confianza al \\(95\\%\\) será el dado por \\(\\pm 2/\\sqrt{T}\\), en el cual se encuentra la mayoría de los coeficientes de autocorrelación estimados. Ahora discutamos algunos ejemplos o aplicaciones. Cuando se realiza la evaluación de la estimación de un modelo de series de tiempo es importante saber si los residuales del modelo realmente tienen propiedades de un proceso puramente aleatorio, en partícular, si ellos no están correlacionados entre sí. Así, la hipotésis a probar será: \\[\\begin{equation} H_0 : \\rho(\\tau) = 0 \\mbox{, para todo } \\tau = 1, 2, \\ldots, m \\mbox{ y } m &lt; T \\end{equation}\\] Esta expresión se puede interpretar como una prueba respecto de si la correlación entre la información de periodos atrás es cero con la información contemporánea. Para hacer una pruena global de la hipotésis de sí un número \\(m\\) de coeficientes de autocovarianzas son cero Box y Pierce (1970) desarrollarón la siguiente estadística: \\[\\begin{equation} Q^* = T \\sum_{j = 1}^{m} \\hat{\\rho} (j)^2 \\end{equation}\\] Bajo la hipotésis nula esta estadística se distribulle asintóticamente como una chi cuadrado (\\(\\chi^2\\)) con \\(m-k\\) grados de libertad y con \\(k\\) que representa al número de paramétros estimados. Haciendo una aplicación estricta de la distribución de esta estadística, sabemos que esta se mantiene asintóticamente. Greta, Ljung y Box (1978) propusieron la siguiente modificación de la estadística para muestras pequeñas: \\[\\begin{equation} Q = T(T + 2) \\sum_{j = 1}^{m} \\frac{\\hat{\\rho} (j)^2}{T - j} \\end{equation}\\] La cual también se distribulle asintóticamente como \\(\\chi^2\\) con \\(m-k\\) grados de libertad. También es intuitivamente claro que la hipótesis nula de no autocorrelación de residuales debería ser rechazada si alguno de los valores \\(\\hat{\\rho} (j)\\) es muy grande, es decir, si \\(Q\\) o \\(Q^*\\) es muy grande. O más precisamente, si estas estadísticas son más grandes que los correspondientes valores críticos de la distribución \\(\\chi^2\\) con \\(m-k\\) grados de libertad a algún grado dado de signficancia. Una alternativa para esta prueba es una del tipo Multiplicadores de Lagrange (o LM) desarrollada por Breusch (1978) y Godfrey (1978). La cual, al igual que las estadísticas \\(Q\\) y \\(Q^*\\), la hipotesis nula está dada por: La prueba consiste en realizar una regresión auxiliar en la cual los residuales se estiman en función de las variables explicativas del modelo original y en los residuales mismos pero rezagados hasta el término \\(m\\) (regresión auxiliar). La prueba resulta en una estadìstica con una distribución \\(\\chi^2\\) con \\(m\\) grados de libertad la cual está dada por la expresión: \\[\\begin{equation} LM = T \\times R^2 \\end{equation}\\] Donde \\(R^2\\) es el resultante de la regresión auxiliar y \\(T\\) es el número de observaciones totales. En comparación con una prueba Durbin - Watson que es comúnmente usada en la econometría tradicional, para probar autocorrelación de los residuales, las estadísticas \\(Q\\), \\(Q^*\\) y \\(LM\\) tienen las siguientes ventajas: El hecho de los residuales no estén autocorrelacionados no implica que estos sean independientes y normalmente distribuidos. La ausencia de autocorrelación no implica una independencia estocástica si las variables son normalmente distribuidas. A menudo se asume que estos residuales están distribuidos normalmente, ya que la mayoría de las pruebas estadísticas tienen este supuesto detrás. No obstante, ello también depende de los otros momentos de la distribución, específicamente del tercer y cuarto momento. Los cuales expresan como: \\[\\begin{equation*} \\mathbb{E}[(X_t - \\mathbb{E}[X_t])^i] \\mbox{, } i = 3, 4 \\end{equation*}\\] El tercer momento es necesario para determinar el sesgo, el cual esta dado como: \\[\\begin{equation} \\hat{S} = \\frac{1}{T} \\frac{\\sum_{t = 1}^{T} (X_t - \\hat{\\mu})^3}{\\sqrt{\\hat{\\gamma}(0)^3}} \\end{equation}\\] Para distribuciones simetricas (como en el caso de la distribución normal) el valor teórico para el sesgo es cero. La curtosis, la cual esta dada en función del cuarto momento, se puede expresar como: \\[\\begin{equation} \\hat{K} = \\frac{1}{T} \\frac{\\sum_{t = 1}^{T} (X_t - \\hat{\\mu})^4}{\\hat{\\gamma}(0)^2} \\end{equation}\\] Para el caso de una distribución normal, esta estadística toma el valor de 3. Valores más grandes que 3 indican que la distribución tienen colas anchas. En tales casos se ubican a los datos financieros. Usando el valor de las estadísticas para medir el sesgo y la curtosis, \\(S\\) y \\(K\\), respectivamente, Jarque y Bera (1980) propusieron una prueba de normalidad, la cual puede ser aplicada a series de tiempo en niveles o en diferencias indistintamente. Dicha prueba se expresa como: \\[\\begin{equation} JB = \\frac{T}{6} \\left(\\hat{S} + \\frac{1}{4} (\\hat{K} - 3)^2 \\right) \\end{equation}\\] La cual tiene una distribución \\(\\chi^2\\) con \\(2\\) grados de libertad y donde \\(T\\) es el tamaño de la muestra. La hipótesis de que las observaciones están distribuidas de forma normal se rechaza si los valores de la estadística de prueba es más grande que los correspondientes valores criticos en tablas. Veamos un ejemplo para ilustrar el uso de la función de autocorrelación. Tomemos como variable al número de pasajeros transportados por el sistema de transporte del metro de la CDMX. Los datos empleados fueron tomados del INEGI y son una serie de tiempo en el periodo que va de enero de 2000 a junio de 2019, es decir, 234 observaciones. Como se puede apreciar en la Figura , el número de pasajeros por mes ha oscilado significativamente a lo largo de tiempo. Incluso podemos observar un cambio estructural de la serie entre 2011 y 2012. Asimismo, podemos ubicar una caida atípica que ocurrió en septiembre de 2017. A esta serie de tiempo le calculamos los pincipales estadísticos hasta ahora estudiados y obtenemos el Cuadro . En dicho cuadro se destaca que se muestra la función de autocirrelación para los tres primeros rezagos. Para mayor detalle, en la Figura se muestra la función de autocorrelación, en donde las bandas descritas por las líneas azules son el intervalo de confianza desntro de las cuales no se puede rechazar la hipotésis nula de que \\(H_0: \\hat{\\rho}(p) = 0\\), para todo \\(\\tau = 1, 2, \\ldots, T-1\\). "],["procesos-estacionarios-univariados.html", "Capítulo 4 Procesos estacionarios univariados", " Capítulo 4 Procesos estacionarios univariados "],["desestacionalización-y-filtrado-de-series.html", "Capítulo 5 Desestacionalización y filtrado de Series", " Capítulo 5 Desestacionalización y filtrado de Series "],["procesos-basados-en-vectores-autoregresivos.html", "Capítulo 6 Procesos Basados en Vectores Autoregresivos", " Capítulo 6 Procesos Basados en Vectores Autoregresivos "],["cointegración.html", "Capítulo 7 Cointegración", " Capítulo 7 Cointegración "],["modelos-multivariados-de-volatilidad-m-arch-y-m-garch.html", "Capítulo 8 Modelos multivariados de volatilidad: \\(M - ARCH\\) y \\(M - GARCH\\)", " Capítulo 8 Modelos multivariados de volatilidad: \\(M - ARCH\\) y \\(M - GARCH\\) "],["modelos-univariados-y-multivariados-de-volatilidad.html", "Capítulo 9 Modelos Univariados y Multivariados de Volatilidad", " Capítulo 9 Modelos Univariados y Multivariados de Volatilidad "],["modelos-adrl.html", "Capítulo 10 Modelos ADRL", " Capítulo 10 Modelos ADRL "],["modelos-de-datos-panel.html", "Capítulo 11 Modelos de Datos Panel", " Capítulo 11 Modelos de Datos Panel "],["otros-modelos-de-series-de-tiempo-no-lineales.html", "Capítulo 12 Otros Modelos de Series de Tiempo No lineales", " Capítulo 12 Otros Modelos de Series de Tiempo No lineales "],["apendice-i.html", "Capítulo 13 Apendice I", " Capítulo 13 Apendice I "]]
