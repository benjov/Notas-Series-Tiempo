<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 13 Apendice I | Notas de Clase: Series de Tiempo</title>
  <meta name="description" content="Trabajo siempre en proceso de mejora, para cualquier comentario o aclaración, contactar al correo benjov@ciencias.unam.mx o omarxalpha@gmail.com" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 13 Apendice I | Notas de Clase: Series de Tiempo" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Trabajo siempre en proceso de mejora, para cualquier comentario o aclaración, contactar al correo benjov@ciencias.unam.mx o omarxalpha@gmail.com" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 13 Apendice I | Notas de Clase: Series de Tiempo" />
  
  <meta name="twitter:description" content="Trabajo siempre en proceso de mejora, para cualquier comentario o aclaración, contactar al correo benjov@ciencias.unam.mx o omarxalpha@gmail.com" />
  

<meta name="author" content="Benjamín Oliva &amp; Omar Alfaro-Rivera" />


<meta name="date" content="2021-08-31" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="otros-modelos-de-series-de-tiempo-no-lineales.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Series de Tiempo</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introducción</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#a-la-naturaleza-de-los-datos-de-series-de-tiempo"><i class="fa fa-check"></i><b>1.1</b> a) La naturaleza de los datos de Series de Tiempo</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#b-ejemplos-y-aplicaciones-de-las-series-de-tiempo"><i class="fa fa-check"></i><b>1.2</b> b) Ejemplos y aplicaciones de las Series de Tiempo</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="elementos-de-ecuaciones-en-diferencia.html"><a href="elementos-de-ecuaciones-en-diferencia.html"><i class="fa fa-check"></i><b>2</b> Elementos de Ecuaciones en Diferencia</a><ul>
<li class="chapter" data-level="2.1" data-path="elementos-de-ecuaciones-en-diferencia.html"><a href="elementos-de-ecuaciones-en-diferencia.html#a-ecuaciones-en-diferencia-para-procesos-deterministas"><i class="fa fa-check"></i><b>2.1</b> a) Ecuaciones en Diferencia para procesos deterministas</a><ul>
<li class="chapter" data-level="2.1.1" data-path="elementos-de-ecuaciones-en-diferencia.html"><a href="elementos-de-ecuaciones-en-diferencia.html#ecuaciones-en-diferencia-lineales-de-primer-orden"><i class="fa fa-check"></i><b>2.1.1</b> Ecuaciones en Diferencia Lineales de Primer Orden</a></li>
<li class="chapter" data-level="2.1.2" data-path="elementos-de-ecuaciones-en-diferencia.html"><a href="elementos-de-ecuaciones-en-diferencia.html#ecuaciones-en-diferencia-lineales-de-segundo-orden-y-de-orden-superior"><i class="fa fa-check"></i><b>2.1.2</b> Ecuaciones en Diferencia Lineales de Segundo Orden y de orden superior</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="elementos-de-ecuaciones-en-diferencia.html"><a href="elementos-de-ecuaciones-en-diferencia.html#operador-de-rezago-l"><i class="fa fa-check"></i><b>2.2</b> Operador de rezago L</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="modelos-de-series-de-tiempo-estacionarias.html"><a href="modelos-de-series-de-tiempo-estacionarias.html"><i class="fa fa-check"></i><b>3</b> Modelos de Series de Tiempo Estacionarias</a><ul>
<li class="chapter" data-level="3.1" data-path="modelos-de-series-de-tiempo-estacionarias.html"><a href="modelos-de-series-de-tiempo-estacionarias.html#definición-de-ergodicidad-y-estacionariedad"><i class="fa fa-check"></i><b>3.1</b> Definición de ergodicidad y estacionariedad</a></li>
<li class="chapter" data-level="3.2" data-path="modelos-de-series-de-tiempo-estacionarias.html"><a href="modelos-de-series-de-tiempo-estacionarias.html#función-de-autocorrelación"><i class="fa fa-check"></i><b>3.2</b> Función de autocorrelación</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html"><i class="fa fa-check"></i><b>4</b> Procesos estacionarios univariados</a><ul>
<li class="chapter" data-level="4.1" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#procesos-autoregresivos-ar"><i class="fa fa-check"></i><b>4.1</b> Procesos Autoregresivos (AR)</a><ul>
<li class="chapter" data-level="4.1.1" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#ar1"><i class="fa fa-check"></i><b>4.1.1</b> AR(1)</a></li>
<li class="chapter" data-level="4.1.2" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#ar2"><i class="fa fa-check"></i><b>4.1.2</b> AR(2)</a></li>
<li class="chapter" data-level="4.1.3" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#arp"><i class="fa fa-check"></i><b>4.1.3</b> AR(p)</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#procesos-de-medias-móviles-ma"><i class="fa fa-check"></i><b>4.2</b> Procesos de Medias Móviles (MA)</a><ul>
<li class="chapter" data-level="4.2.1" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#ma1"><i class="fa fa-check"></i><b>4.2.1</b> MA(1)</a></li>
<li class="chapter" data-level="4.2.2" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#maq"><i class="fa fa-check"></i><b>4.2.2</b> MA(q)</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#procesos-armap-q-y-arimap-d-q"><i class="fa fa-check"></i><b>4.3</b> Procesos ARMA(p, q) y ARIMA(p, d, q)</a><ul>
<li class="chapter" data-level="4.3.1" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#arma1-1"><i class="fa fa-check"></i><b>4.3.1</b> ARMA(1, 1)</a></li>
<li class="chapter" data-level="4.3.2" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#armap-q"><i class="fa fa-check"></i><b>4.3.2</b> ARMA(p, q)</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#función-de-autocorrelación-parcial"><i class="fa fa-check"></i><b>4.4</b> Función de Autocorrelación Parcial</a></li>
<li class="chapter" data-level="4.5" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#selección-de-las-constantes-p-q-d-en-un-arp-un-maq-un-armap-q-o-un-arimap-d-q"><i class="fa fa-check"></i><b>4.5</b> Selección de las constantes p, q, d en un AR(p), un MA(q), un ARMA(p, q) o un ARIMA(p, d, q)</a></li>
<li class="chapter" data-level="4.6" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#pronósticos"><i class="fa fa-check"></i><b>4.6</b> Pronósticos</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="desestacionalización-y-filtrado-de-series.html"><a href="desestacionalización-y-filtrado-de-series.html"><i class="fa fa-check"></i><b>5</b> Desestacionalización y filtrado de Series</a></li>
<li class="chapter" data-level="6" data-path="procesos-basados-en-vectores-autoregresivos.html"><a href="procesos-basados-en-vectores-autoregresivos.html"><i class="fa fa-check"></i><b>6</b> Procesos Basados en Vectores Autoregresivos</a></li>
<li class="chapter" data-level="7" data-path="cointegración.html"><a href="cointegración.html"><i class="fa fa-check"></i><b>7</b> Cointegración</a></li>
<li class="chapter" data-level="8" data-path="modelos-multivariados-de-volatilidad-m-arch-y-m-garch.html"><a href="modelos-multivariados-de-volatilidad-m-arch-y-m-garch.html"><i class="fa fa-check"></i><b>8</b> Modelos multivariados de volatilidad: <span class="math inline">\(M - ARCH\)</span> y <span class="math inline">\(M - GARCH\)</span></a></li>
<li class="chapter" data-level="9" data-path="modelos-univariados-y-multivariados-de-volatilidad.html"><a href="modelos-univariados-y-multivariados-de-volatilidad.html"><i class="fa fa-check"></i><b>9</b> Modelos Univariados y Multivariados de Volatilidad</a></li>
<li class="chapter" data-level="10" data-path="modelos-adrl.html"><a href="modelos-adrl.html"><i class="fa fa-check"></i><b>10</b> Modelos ADRL</a></li>
<li class="chapter" data-level="11" data-path="modelos-de-datos-panel.html"><a href="modelos-de-datos-panel.html"><i class="fa fa-check"></i><b>11</b> Modelos de Datos Panel</a></li>
<li class="chapter" data-level="12" data-path="otros-modelos-de-series-de-tiempo-no-lineales.html"><a href="otros-modelos-de-series-de-tiempo-no-lineales.html"><i class="fa fa-check"></i><b>12</b> Otros Modelos de Series de Tiempo No lineales</a></li>
<li class="chapter" data-level="13" data-path="apendice-i.html"><a href="apendice-i.html"><i class="fa fa-check"></i><b>13</b> Apendice I</a><ul>
<li class="chapter" data-level="13.1" data-path="apendice-i.html"><a href="apendice-i.html#estimador-de-mínimos-cuadrados-ordinarios-y-el-análisis-clásico-de-regresión"><i class="fa fa-check"></i><b>13.1</b> Estimador de Mínimos Cuadrados Ordinarios y el análisis clásico de regresión</a></li>
<li class="chapter" data-level="13.2" data-path="apendice-i.html"><a href="apendice-i.html#estimación-por-el-método-de-máxima-verosimilitud-mv"><i class="fa fa-check"></i><b>13.2</b> Estimación por el método de Máxima Verosimilitud (MV)</a></li>
<li class="chapter" data-level="13.3" data-path="apendice-i.html"><a href="apendice-i.html#métricas-de-bondad-de-ajuste"><i class="fa fa-check"></i><b>13.3</b> Métricas de bondad de ajuste</a></li>
<li class="chapter" data-level="13.4" data-path="apendice-i.html"><a href="apendice-i.html#pruebas-de-hipótesis"><i class="fa fa-check"></i><b>13.4</b> Pruebas de Hipótesis</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/benjov/Series-de-Tiempo-2021" target="blank">Repositorio del Curso</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notas de Clase: Series de Tiempo</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="apendice-i" class="section level1">
<h1><span class="header-section-number">Capítulo 13</span> Apendice I</h1>
<div id="estimador-de-mínimos-cuadrados-ordinarios-y-el-análisis-clásico-de-regresión" class="section level2">
<h2><span class="header-section-number">13.1</span> Estimador de Mínimos Cuadrados Ordinarios y el análisis clásico de regresión</h2>
<p>El estimador de Mínimos Cuadrados Ordinarios (MCO) es el estimador básico en econometría o, propiamente dicho, en el análisis de regresión. Esta sección cubre las propiedades finitas del estimador de MCO, mismas que son validas para cualquier tamaño de muestra dado. El material cubierto es totalmente estándar.</p>
<p>Cualquier estudio econométrico inicia con un conjunto de proposiciones sobre algún fenómeno de la economía. Algunos ejemplos familiares son las ecuaciones de demanda, las funciones de producción y algunos otros modelos macroeconómicos. Así, la investigación empírica provee las estimaciones de los parámetros desconocidos en el modelo. La teoría especifica un conjunto de relaciones determinísticas sobre las variables.</p>
<p>Dichas relaciones sulen estudiarse mediante el análisis de regresión múltiple, el cual permite el estudio de la relación entre una <em>variable dependiente</em> y otras más denominadas <em>variables independientes</em>. En adelante, en general diremos que la forma de representar la relación entre la variable dependiente y las variables independientes, tendrá la siguiente notación:
<span class="math display">\[\begin{eqnarray}
    y &amp; = &amp; f(x_1, x_2, \ldots , x_K) + \varepsilon \nonumber \\
    &amp; = &amp; x_1\beta_1 + x_2\beta_2 + \ldots + x_K\beta_K + \varepsilon
\end{eqnarray}\]</span></p>
<p>donde <span class="math inline">\(y\)</span> es la variable dependiente o <em>explicada</em>, el conjunto de variables dado por <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, …, <span class="math inline">\(x_K\)</span> son las variables independientes o <em>explicativas</em> y de la teoría tomamos la especificación descrita por <span class="math inline">\(f(x_1, x_2, \ldots, x_K)\)</span>. Esta función es comúnmente llamada la <em>ecuación de regresión poblacional</em> de <span class="math inline">\(y\)</span> en <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, …, <span class="math inline">\(x_K\)</span>. El término <span class="math inline">\(\varepsilon\)</span> es una <em>perturbación</em> aleatoria o error de estimación.</p>
<p>Este error existe por varias razones, principalmente, porque no esperamos capturar toda la influencia que existe o determina a una varaible económica en un modelo simplista como el que generalmente se formula en el análisis de regresión. Digamos, entonces, que existe un conjunto de información no observable que permite la existencia del término de error. Por ejemplo, existe una clara dificultad para obtener medidas razonables de cualidades como habilidades o capcidades de aprendizaje de un conjunto de individuos a los cuales, quizá, queremos medir su productividad. Por lo tanto, sólo podemos medir el efecto de aquellas variables o información que es cuantificable. El resto de la información la conoceremos como aquella que no es observable. Así, el término de error existe a razón de dicha información.</p>
<p>Implícitamente, estamos suponiendo que cada una de las observaciones en una muestra dada por <span class="math inline">\(\{y_i, x_{i1}, x_{i2}, \ldots, x_{iK} \}\)</span>, para <span class="math inline">\(i = 1, \ldots, n\)</span>, fue generada por un proceso subyacente descrito por:
<span class="math display">\[\begin{equation}
    y_i = x_{i1}\beta_1 + x_{i2}\beta_2 + \ldots + x_{iK}\beta_K + \varepsilon_i
\end{equation}\]</span></p>
<p>Es decir, el valor observado de <span class="math inline">\(y_i\)</span> es igual a la suma de dos partes: una parte determinística, <span class="math inline">\(x_{i1}\beta_1 + x_{i2}\beta_2 + \ldots + x_{iK}\beta_K\)</span>, y una parte aleatoria, <span class="math inline">\(\varepsilon_i\)</span>. Dicho esto, el objetivo del análisis de regresión radica en estimar los parámetros desconocidos del modelo (<span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\beta_K\)</span>), validar la proposiciones teóricas usando los datos disponibles y predecir el valor de la variable <span class="math inline">\(y_i\)</span> mediante el uso del modelo estimado.</p>
<p>Sea <span class="math inline">\(\mathbf{X}_k\)</span> el vector columna de <span class="math inline">\(n\)</span> observaciones de la variable <span class="math inline">\(x_k\)</span>, donde <span class="math inline">\(k = 1, \ldots, K\)</span>, y que colocado en una matriz da por resultado un arreglo <span class="math inline">\(\mathbf{X}\)</span> de tamaño <span class="math inline">\(n \times K\)</span>. Es decir, cada una de las columnas de la siguiente matriz representa todas las observaciones de cada una de las variables:
<span class="math display">\[\begin{equation}
    \left[ 
    \begin{array}{c c c c}
    \mathbf{X}_1 &amp; \mathbf{X}_2 &amp; \ldots &amp; \mathbf{X}_K 
    \end{array} 
    \right]
    = 
    \left[ 
    \begin{array}{c c c c}
    x_{11} &amp; x_{12} &amp; \ldots &amp; x_{1K}\\
    x_{21} &amp; x_{22} &amp; \ldots &amp; x_{2K}\\
    x_{31} &amp; x_{32} &amp; \ldots &amp; x_{3K}\\
    \vdots &amp; \vdots &amp; \ldots &amp; \vdots\\
    x_{n1} &amp; x_{n2} &amp; \ldots &amp; x_{nK}\\
    \end{array} 
    \right]
\end{equation}\]</span></p>
<p>En la mayoría de las veces vamos a asumir que existe una columna compuesta del número 1 (uno) en todas sus entradas, tal que, el paramétro <span class="math inline">\(\beta_1\)</span> es un término constante en el modelo. De esta forma la matriz anteriormente mostrada se puede ver como:
<span class="math display">\[\begin{equation}
    \left[ 
    \begin{array}{c c c c}
    \mathbf{1} &amp; \mathbf{X}_2 &amp; \ldots &amp; \mathbf{X}_K 
    \end{array} 
    \right]
    = 
    \left[ 
    \begin{array}{c c c c}
    1 &amp; x_{12} &amp; \ldots &amp; x_{1K}\\
    1 &amp; x_{22} &amp; \ldots &amp; x_{2K}\\
    1 &amp; x_{32} &amp; \ldots &amp; x_{3K}\\
    \vdots &amp; \vdots &amp; \ldots &amp; \vdots\\
    1 &amp; x_{n2} &amp; \ldots &amp; x_{nK}\\
    \end{array} 
    \right]
\end{equation}\]</span></p>
<p>Adicionalmente, denotaremos a <span class="math inline">\(\mathbf{Y}\)</span> como un vector columna de <span class="math inline">\(n\)</span> observaciones (<span class="math inline">\(y_1\)</span>, <span class="math inline">\(y_2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(y_n\)</span>, en forma de columna), y a <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> como el vector columna de <span class="math inline">\(n\)</span> perturbaciones (<span class="math inline">\(\varepsilon_1\)</span>, <span class="math inline">\(\varepsilon_2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\varepsilon_n\)</span>, en forma de columna). El modelo descrito en la ecuaci'on (1) se puede escribir en su forma general como:
<span class="math display">\[\begin{equation}
{\bf{Y}} = {\bf {X}}_1\beta_1 + {\bf{X}}_2\beta_2 + \ldots + {\bf{X}}_K\beta_K + {\boldsymbol{\varepsilon}}
\end{equation}\]</span>
\</p>
<p>Ecuación que podemos rescribir como:
<span class="math display">\[\begin{equation}
\mathbf{Y} 
=
\left[ 
\begin{array}{c c c c}
\mathbf{X}_1 &amp; \mathbf{X}_2 &amp; \ldots &amp; \mathbf{X}_K 
\end{array} 
\right]
\left[ 
\begin{array}{c}
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_K 
\end{array} 
\right]
+
\
\boldsymbol{\varepsilon}
= 
\boldsymbol{X\beta} + \boldsymbol{\varepsilon}
\end{equation}\]</span></p>
<p>Adicionalmente, de ahora en delante diremos que la regresión lineal dada por <span class="math inline">\(y_i = x_{i1}\beta_1 + x_{i2}\beta_2 + \ldots + x_{iK}\beta_K + \varepsilon_i\)</span>, se podrá escribir como:
<span class="math display">\[\begin{equation}
y_i = (x_{i1}, x_{i2}, \ldots, x_{iK})(\beta_1, \beta_2, \ldots, \beta_K)&#39; + \varepsilon = \mathbf{X}&#39;_i \boldsymbol{\beta} + \varepsilon_i
\end{equation}\]</span></p>
<p>Así, los parámetros desconocidos, <span class="math inline">\(\boldsymbol{\beta}\)</span>, de la relación estocástica dada por <span class="math inline">\(y_i = \mathbf{X}&#39;_i \boldsymbol{\beta} + \varepsilon_i\)</span> son el objeto de la estimación. En este sentido distingamos que <span class="math inline">\(\boldsymbol{\beta}\)</span> y <span class="math inline">\(\varepsilon_i\)</span> son, respectivamente, el conjumto de los parámetros y el término de error de la población, y que, por lo tanto, denotaremos a las estimaciones relsultantes de una muestra como <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> y <span class="math inline">\(e_i\)</span>. Es decir, siempre que no podamos adquirir o conocer la iformación de todos los elementos de la población, nuestras aproximaciones muestrales se denotarán de forma distinta a las que refieran a la población.</p>
<p>Así, los principios de  y  están dados por las fórmulas <span class="math inline">\(E[y_i|\mathbf{X}_i] = \mathbf{X}&#39;_i\boldsymbol{\beta}\)</span> y <span class="math inline">\(\hat{y}_i = \mathbf{X}&#39;_i\hat{\boldsymbol{\beta}}\)</span>, respectivamente. Donde <span class="math inline">\(\hat{y}_i\)</span> es el estimador de <span class="math inline">\(E[y_i|\mathbf{X}_i]\)</span>.</p>
<p>Por su parte, el término de error asociado será:
<span class="math display">\[\begin{equation}
\varepsilon_i = y_i - {\bf{X}}&#39;_i \boldsymbol{\beta}
\end{equation}\]</span></p>
<p>si hablamos del caso poblacional o,
<span class="math display">\[\begin{equation}
e_i = y_i - {\bf{X}}&#39;_i \hat{\boldsymbol{\beta}}
\end{equation}\]</span></p>
<p>cuando hagamos referencia al caso muestral. Es decir, nuestro estimador de <span class="math inline">\(\varepsilon_i\)</span> es <span class="math inline">\(e_i\)</span>. De lo dicho hasta ahora podemos escribir:
<span class="math display">\[\begin{equation}
y_i = \mathbf{X}&#39;_i \boldsymbol{\beta} + \varepsilon_i = \mathbf{X}&#39;_i \hat{\boldsymbol{\beta}} + e_i
\end{equation}\]</span></p>
<p>Intuitivamente, la ecuación (36) significa que siempre que poseamos una muestra de los elementos de la población, podremos explicar una parte de la variable dependiente, no su totalidad. En este sentido, el análisis de regresión consiste en un proceso de ajuste a la variable dependiente. Está es la idea que da origen al <span class="math inline">\(R^2\)</span> y otras medidas de bondad de ajuste, mismas que más adelante en el curso analizaremos.</p>
<p>Regresando a la discusión central de esta sección, el método de MCO, en consecuencia, resulta en encontrar la combinación de parámetros <span class="math inline">\(\hat\beta\)</span> que permita minimizar la suma de los residuales al cuadrado dada por:
<span class="math display">\[\begin{equation}
\sum^{n}_{i=1}{e^2_i} = \sum^{n}_{i=1}{(y_i - {\bf X}&#39;_i\hat{\boldsymbol{\beta}})^2}
\end{equation}\]</span></p>
<p>donde <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> denota el vector de estimadores <span class="math inline">\(\hat{\beta}_1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\hat{\beta}_K\)</span>. En términos matriciales, dado que <span class="math inline">\((e_1, e_2, \ldots, e_n)&#39;(e_1, e_2, \ldots, e_n) = {\mathbf{e&#39;e}}\)</span>, el problema del método de MCO consiste en:
<span class="math display">\[\begin{equation}
Minimizar_{\hat{\boldsymbol \beta}} S(\hat{\boldsymbol \beta}) = Minimizar_{\hat{\boldsymbol \beta}} \mathbf{e&#39;e} 
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
= Minimizar_{\hat{\boldsymbol \beta}} (\mathbf{Y}-\mathbf{X}\hat{\boldsymbol \beta})&#39;(\mathbf{Y}-\mathbf{X}\hat{\boldsymbol \beta})
\end{equation}\]</span></p>
<p>Expandiendo la expresión <span class="math inline">\(\mathbf{e&#39;e}\)</span> obtenemos:
<span class="math display">\[\begin{equation}
\mathbf{e&#39;e} = \mathbf{Y&#39;Y} - 2 \mathbf{Y&#39;X} \hat{\boldsymbol \beta} + \hat{\boldsymbol \beta}&#39; \mathbf{X&#39;X}\hat{\boldsymbol \beta}
\end{equation}\]</span></p>
<p>De esta forma obtenemos que las condiciones necesarias de un mínimo son:
<span class="math display">\[\begin{equation}
\frac{\partial S(\hat{\boldsymbol \beta})}{\partial \hat{\boldsymbol \beta}} = -2{\bf{X&#39;Y}} + 2{\bf{X&#39;X}} \hat{\boldsymbol{\beta}} = \bf{0}
\end{equation}\]</span></p>
<p>De ecuación anterior obtenemos para la solución del problema del mínimo a las ecuaciones siguientes conocidas como  dadas por:
<span class="math display">\[\begin{equation}
\bf{X&#39;X}\hat{\boldsymbol \beta} = \bf{X&#39;Y}
\end{equation}\]</span></p>
<p>Notemos que dichas ecuaciones normales son en realidad un sistema de ecuaciones de <span class="math inline">\(K\)</span> variables o incógnitas. Por un lado, recordemos que <span class="math inline">\(\mathbf X\)</span> es una matriz de dimensión <span class="math inline">\(n \times K\)</span>, con lo cual <span class="math inline">\(\mathbf X&#39;\)</span> es de dimensión <span class="math inline">\(K \times n\)</span>. Así, el producto <span class="math inline">\(\mathbf{X&#39;X}\)</span> dará como resultado una matriz cuadrada de dimensión <span class="math inline">\(K \times K\)</span>. Por otro lado, sabemos que <span class="math inline">\(\mathbf Y\)</span> es un vector de tamaño <span class="math inline">\(n \times 1\)</span>, con lo cual el producto <span class="math inline">\(\mathbf{X&#39;Y}\)</span> da como resultado un vector de dimención <span class="math inline">\(K \times 1\)</span>. En conclusión, el sistema de ecuaciones normales consiste en <span class="math inline">\(K\)</span> ecuaciones con <span class="math inline">\(K\)</span> incógnitas (<span class="math inline">\(\hat{\beta}_1, \ldots, \hat{\beta}_K\)</span>). Ante este hecho, existen múltiples formas mediante las cuales se puede solucionar dicho sistema, sin embargo en nuesto caso seguiremos el siguiente procedimiento.</p>
<p>Si la inversa de la matriz <span class="math inline">\(\mathbf{X&#39;X}\)</span> existe (recuerde que el procedimiento de MCO tradicional supone que <span class="math inline">\(\mathbf{X}\)</span> es de rango completo), la solución esta dada por la siguiente expresión:
<span class="math display">\[\begin{equation}
\hat{\boldsymbol \beta} = (\mathbf{X&#39;X})^{-1}\mathbf{X&#39;Y}
\end{equation}\]</span></p>
<p>Esta expresión, a pesar de ser en apariencia compleja se puede ver como un conjunto de sumas. En general hemos supuesto que nuestra regresión a estimar esta descrita por la eccuación: <span class="math inline">\(y_i = x_{i1}\beta_1 + x_{i2}\beta_2 + \ldots + x_{iK}\beta_K + \varepsilon_i\)</span>, de esta forma tenemos <span class="math inline">\(K\)</span> variables independientes es nuestra regresión.</p>
<p>Ahora bien, si denotamos a <span class="math inline">\(\mathbf{X}_k\)</span> como el vector columna formado por todas las observaciones de la muestra (<span class="math inline">\(i = 1, 2, \ldots, n\)</span>) para la variable <span class="math inline">\(k\)</span>, podemos decir que la matriz <span class="math inline">\(\mathbf X\)</span> que contiene todas las variable independientes se forma por la concatenación de cada uno de los <span class="math inline">\(K\)</span> vectores columna. Dicho esto, podemos ver que las matrices <span class="math inline">\(\mathbf{X}\)</span> y <span class="math inline">\(\mathbf{X&#39;}\)</span> se pueden expresar como:</p>
<p><span class="math display">\[
\mathbf{X} = 
\left[ \begin{array}{ccccc}
x_{11} &amp; x_{12} &amp; x_{13} &amp; \ldots &amp; x_{1K} \\
x_{21} &amp; x_{22} &amp; x_{23} &amp; \ldots &amp; x_{2K} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
x_{n1} &amp; x_{n2} &amp; x_{n3} &amp; \ldots &amp; x_{nK}
\end{array} \right] =
\left[ \begin{array}{ccccc}
\mathbf{X}_1 &amp; \mathbf{X}_2&amp; \mathbf{X}_3 &amp; \ldots &amp; \mathbf{X}_K
\end{array} \right]
\]</span></p>
<p><span class="math display">\[
\bf{X&#39;} = 
\left[ \begin{array}{ccccc}
x_{11} &amp; x_{21} &amp; x_{31} &amp; \ldots &amp; x_{n1} \\
x_{12} &amp; x_{22} &amp; x_{32} &amp; \ldots &amp; x_{n2} \\
x_{13} &amp; x_{23} &amp; x_{33} &amp; \ldots &amp; x_{n3} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
x_{1K} &amp; x_{2K} &amp; x_{3K} &amp; \ldots &amp; x_{nK}
\end{array} \right] =
\left[ \begin{array}{c}
\mathbf{X}_1&#39; \\
\mathbf{X}_2&#39;\\
\mathbf{X}_3&#39;\\
\vdots \\
\mathbf{X}_K&#39;
\end{array} \right]
\]</span>\</p>
<p>Si suponemos que nuestra regresión tiene una constante, la especificación sería: <span class="math inline">\(y_i = \beta_1 + x_{i2}\beta_2 + \ldots + x_{iK}\beta_K + \varepsilon_i\)</span>, con unas matrices <span class="math inline">\(\mathbf X\)</span> y <span class="math inline">\(\mathbf X&#39;\)</span> dadas:
<span class="math display">\[
\mathbf{X} = 
\left[ \begin{array}{ccccc}
1 &amp; x_{12} &amp; x_{13} &amp; \ldots &amp; x_{1K} \\
1 &amp; x_{22} &amp; x_{23} &amp; \ldots &amp; x_{2K} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
1 &amp; x_{n2} &amp; x_{n3} &amp; \ldots &amp; x_{nK}
\end{array} \right] =
\left[ \begin{array}{ccccc}
\mathbf{1_n} &amp; \mathbf{X}_2&amp; \mathbf{X}_3 &amp; \ldots &amp; \mathbf{X}_K
\end{array} \right]
\]</span></p>
<p><span class="math display">\[
\bf{X&#39;} = 
\left[ \begin{array}{ccccc}
1 &amp; 1 &amp; 1 &amp; \ldots &amp; 1 \\
x_{12} &amp; x_{22} &amp; x_{32} &amp; \ldots &amp; x_{n2} \\
x_{13} &amp; x_{23} &amp; x_{33} &amp; \ldots &amp; x_{n3} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
x_{1K} &amp; x_{2K} &amp; x_{3K} &amp; \ldots &amp; x_{nK}
\end{array} \right] =
\left[ \begin{array}{c}
\mathbf{1_n}&#39; \\
\mathbf{X}_2&#39;\\
\mathbf{X}_3&#39;\\
\vdots \\
\mathbf{X}_K&#39;
\end{array} \right]
\]</span>\</p>
<p>Donde <span class="math inline">\(\mathbf{1_n}\)</span> es un vector columna compuesto de 1’s (unos). Retomando (14), desarrollemos cada uno de los casos anteriores, así obtenemos lo siguiente para el caso general:
<span class="math display">\[
\mathbf{X&#39;X} = 
\left[ \begin{array}{c}
\mathbf{X}_1&#39; \\
\mathbf{X}_2&#39; \\
\mathbf{X}_3&#39;\\
\vdots \\
\mathbf{X}_K&#39;
\end{array} \right]
\left[ \begin{array}{ccccc}
\mathbf{X}_1 &amp; \mathbf{X}_2 &amp; \mathbf{X}_3 &amp; \ldots &amp; \mathbf{X}_K
\end{array} \right]
\]</span></p>
<p><span class="math display">\[ 
\left[ \begin{array}{ccccc}
\mathbf{X}_1&#39;\bf{X}_1 &amp; \mathbf{X}_1&#39;\mathbf{X}_2 &amp; \mathbf{X}_1&#39;\mathbf{X}_3 &amp; \ldots &amp; \mathbf{X}_1&#39;\mathbf{X}_K \\
\mathbf{X}_2&#39;\mathbf{X}_1 &amp; \mathbf{X}_2&#39;\mathbf{X}_2 &amp; \mathbf{X}_2&#39;\mathbf{X}_3 &amp; \ldots &amp; \mathbf{X}_2&#39;\mathbf{X}_K \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
\mathbf{X}_K&#39;\mathbf{X}_1 &amp; \mathbf{X}_K&#39;\mathbf{X}_2 &amp; \mathbf{X}_K&#39;\mathbf{X}_3 &amp; \ldots &amp; \mathbf{X}_K&#39;\mathbf{X}_K
\end{array} \right]
\]</span>\</p>
<p>Por lo tanto, obtenemos que:</p>
<p><span class="math display">\[
\mathbf{X&#39;X} =
\left[ \begin{array}{ccccc}
\sum^{n}_{i=1}{x_{i1}^2} &amp; \sum^{n}_{i=1}{x_{i1}x_{i2}} &amp; \sum^{n}_{i=1}{x_{i1}x_{i3}} &amp; \ldots &amp; \sum^{n}_{i=1}{x_{i1}x_{iK}}\\
\sum^{n}_{i=1}{x_{i2}x_{i1}} &amp; \sum^{n}_{i=1}{x^{2}_{i2}} &amp; \sum^{n}_{i=1}{x_{i2}x_{i3}} &amp; \ldots &amp; \sum^{n}_{i=1}{x_{i2}x_{iK}}\\
\sum^{n}_{i=1}{x_{i3}x_{i1}} &amp; \sum^{n}_{i=1}{x_{i3}x_{i2}} &amp; \sum^{n}_{i=1}{x^2_{i3}} &amp; \ldots &amp; \sum^{n}_{i=1}{x_{i3}x_{iK}}\\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
\sum^{n}_{i=1}{x_{iK}x_{i1}} &amp; \sum^{n}_{i=1}{x_{iK}x_{i2}} &amp; \sum^{n}_{i=1}{x_{iK}x_{i3}} &amp; \ldots &amp; \sum^{n}_{i=1}{x^2_{iK}}
\end{array} \right]
\]</span>\</p>
<p>Por otro lado, cuando supongamos que existe un término constante:</p>
<p><span class="math display">\[
\mathbf{X&#39;X} =
\left[ \begin{array}{ccccc}
n &amp; \sum^{n}_{i=1}{x_{i2}} &amp; \sum^{n}_{i=1}{x_{i3}} &amp; \ldots &amp; \sum^{n}_{i=1}{x_{iK}}\\
\sum^{n}_{i=1}{x_{i2}} &amp; \sum^{n}_{i=1}{x^{2}_{i2}} &amp; \sum^{n}_{i=1}{x_{i2}x_{i3}} &amp; \ldots &amp; \sum^{n}_{i=1}{x_{i2}x_{iK}}\\
\sum^{n}_{i=1}{x_{i3}} &amp; \sum^{n}_{i=1}{x_{i3}x_{i2}} &amp; \sum^{n}_{i=1}{x^2_{i3}} &amp; \ldots &amp; \sum^{n}_{i=1}{x_{i3}x_{iK}}\\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
\sum^{n}_{i=1}{x_{iK}} &amp; \sum^{n}_{i=1}{x_{iK}x_{i2}} &amp; \sum^{n}_{i=1}{x_{iK}x_{i3}} &amp; \ldots &amp; \sum^{n}_{i=1}{x^2_{iK}}
\end{array} \right]
\]</span>\</p>
<p>Adicionalmente, el producto <span class="math inline">\(\mathbf{X&#39;Y}\)</span>, en el caso general, se puede expresar como:</p>
<p><span class="math display">\[
\mathbf{X&#39;Y} 
= 
\left[ \begin{array}{c}
\mathbf{X}_1&#39; \\
\mathbf{X}_2&#39; \\
\mathbf{X}_3&#39;\\
\vdots \\
\mathbf{X}_K&#39;
\end{array} \right] \mathbf{Y}
= 
\left[ \begin{array}{c}
\mathbf{X}_1&#39;\mathbf{Y}\\
\mathbf{X}_2&#39;\mathbf{Y}\\
\mathbf{X}_3&#39;\mathbf{Y}\\
\vdots\\
\mathbf{X}_K&#39;\mathbf{Y}
\end{array} \right]
=
\left[ \begin{array}{c}
\sum^{n}_{i=1}{x_{i1}y_{i}}\\
\sum^{n}_{i=1}{x_{i2}y_{i}}\\
\sum^{n}_{i=1}{x_{i3}y_{i}}\\
\vdots\\
\sum^{n}_{i=1}{x_{iK}y_{i}}
\end{array} \right]
\]</span>\</p>
<p>Si el modelo supone la existencia de un término constante, dicho producto se expresa como:</p>
<p><span class="math display">\[
\mathbf{X&#39;Y} 
= 
\left[ \begin{array}{c}
\mathbf{1_n}&#39; \\
\mathbf{X}_2&#39; \\
\mathbf{X}_3&#39;\\
\vdots \\
\mathbf{X}_K&#39;
\end{array} \right] \mathbf{Y}
= 
\left[ \begin{array}{c}
\mathbf{1_n}&#39;\mathbf{Y}\\
\mathbf{X}_2&#39;\mathbf{Y}\\
\mathbf{X}_3&#39;\mathbf{Y}\\
\vdots\\
\mathbf{X}_K&#39;\mathbf{Y}
\end{array} \right]
=
\left[ \begin{array}{c}
\sum^{n}_{i=1}{y_{i}}\\
\sum^{n}_{i=1}{x_{i2}y_{i}}\\
\sum^{n}_{i=1}{x_{i3}y_{i}}\\
\vdots\\
\sum^{n}_{i=1}{x_{iK}y_{i}}
\end{array} \right]
\]</span>\</p>
<p>Finalmente, para que esta solución dada para el procedimiento de MCO sea un mínimo debemos buscar las condiciones de segundo orden:
<span class="math display">\[\begin{equation}
\frac{\partial^2 S(\hat{\boldsymbol \beta})}{\partial \hat{\boldsymbol \beta} \partial \hat{\boldsymbol \beta&#39;}} = 2 \mathbf{X&#39;X}
\end{equation}\]</span></p>
<p>donde la matriz <span class="math inline">\(\mathbf{X&#39;X}\)</span> debe ser positiva definida para que la solución de MCO sea un mínimo. Sea <span class="math inline">\(q = \mathbf{c&#39;X&#39;Xc}\)</span> para algún vector <span class="math inline">\(\mathbf{c}\)</span> distinto de cero. Entonces:</p>
<p><span class="math display">\[
q = \mathbf{v&#39;v} = \sum_{i=1}^{n}{v_i^2} \textrm{, donde } \mathbf{v = Xc}
\]</span>\</p>
<p>Así, <span class="math inline">\(q\)</span> es positivo. Si <span class="math inline">\(\mathbf v\)</span> fuera cero, entonces existe una combinación lineal de las columnas de <span class="math inline">\(\mathbf X\)</span> que da como resultado cero, lo cual contradice el supuesto de que <span class="math inline">\(\mathbf X\)</span> es de rango completo. En todos los casos, si <span class="math inline">\(\mathbf X\)</span> es de rango completo, entonces la solución del método de MCO, <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, es la única que mínimiza la suma de los residuales al cuadrado.</p>
<p>Finalmente, no debemos perder de vista que lo aqui espresado tiene un objeto, mostrar como este procedimiento de MCO es valido cuando suponemos regresiones del tipo:
<span class="math display">\[\begin{equation}
y_t = \beta_0 + y_{t-1}\beta_1 + \ldots + y_{t-\tau}\beta_\tau + \varepsilon_t
\end{equation}\]</span></p>
<p>Donde <span class="math inline">\(t = 0, 1, 2, \ldots, T\)</span> es un índice del tiempo, la variable dependiente es <span class="math inline">\(y_t\)</span> y las variables independientes son la misma variable independiente, pero en forma rezagada. Así, podemos definir un vector columna <span class="math inline">\(\mathbf{Y}_{t-k}\)</span> el vector columna de <span class="math inline">\(T\)</span> observaciones de la variable dependiente rezagada <span class="math inline">\(k\)</span> veces, donde <span class="math inline">\(k = 1, \ldots, \tau\)</span>, y que colocado en una matriz da por resultado un arreglo <span class="math inline">\(\mathbf{X}\)</span> de tamaño <span class="math inline">\(T-\tau \times \tau+1\)</span>.</p>
</div>
<div id="estimación-por-el-método-de-máxima-verosimilitud-mv" class="section level2">
<h2><span class="header-section-number">13.2</span> Estimación por el método de Máxima Verosimilitud (MV)</h2>
<p>Ahora analicemos otro método de estimación que tiene más uso en series de tiempo: MV. Iniciemos con algo de notación. La función de densidad de probabilidad de una variable aleatoria <span class="math inline">\(y\)</span>, condicional en un conjunto de parámetros, <span class="math inline">\(\boldsymbol{\theta}\)</span>, la denotaremos como <span class="math inline">\(f(y|\boldsymbol{\theta})\)</span>. Dicha función identifica el mecanismo generador de datos subyacente a la muestra observable y al mismo tiempo prove una descripción matemática de los datos que el proceso generará.</p>
<p>Por otro lado, la función de densidad conjunta de n observaciones  (i.i.d) está dada por el siguiente producto de las funciones de densidad individuales:
<span class="math display">\[\begin{equation}
f(y_1, y_2, \ldots, y_n |\boldsymbol{\theta}) = \prod_{i=1}^n f(y_i|\boldsymbol{\theta}) = L(\boldsymbol{\theta} | \boldsymbol{y})
\end{equation}\]</span></p>
<p>A esta función de densidad conjunta se le conoce como , la cual se define como una función del vector de parámetros, <span class="math inline">\(\boldsymbol{\theta}\)</span>, donde <span class="math inline">\(\boldsymbol{y}\)</span> indica la familia de observaciones en la muestra de datos. Notemos que la función de densidad conjunta la hemos escrito como una función de los datos observados condicional en los parámetros a estimar. Sin embargo, por otro lado también hemos dicho que la función de verosimilitud, aquella que es ídentica a la función de densidad conjunta, es una función de los parámetros condicional en los datos observados. Aunque ambas funciones son la misma cabe hace enfasís de que en la segunda buscamos aquellos parámetros que máximizan la función de verosimilitud condicional en los datos observados.</p>
<p>Ahora bien, el procedimiento de máxima verosimilitud, por simplicidad, se estima aplicando la función logarítmo natural a <span class="math inline">\(L(\boldsymbol{\theta} | \boldsymbol{y})\)</span>. Derivado de que la función logarítmo natural es monótona, ésta preserva el orden y con ello el valorque máximiza a la función. De esta forma escribiremos que la función será:
<span class="math display">\[\begin{equation}
ln(L(\boldsymbol{\theta} | \boldsymbol{y})) = ln(\prod_{i=1}^n f(y_i|\boldsymbol{\theta})) = \sum_{i = 1}^{n} ln(f(y_i|\boldsymbol{\theta}))
\end{equation}\]</span></p>
<p>Así, por simplicidad diremos que denotaremos a el logarítmo de la función de densidad conjunta como:
<span class="math display">\[\begin{equation}
ln(L(\boldsymbol{\theta} | \boldsymbol{y})) = l(\boldsymbol{\theta} | \boldsymbol{y})
\end{equation}\]</span></p>
<p>Dicho lo anterior, el objetivo de esta sección es mostrar el procedimiento de estimación de Máxima Verosimilitud aplicado a una regresión lineal. Retomemos la idea de que en nuestra ecuación de regresión lineal: <span class="math inline">\(y_i = \mathbf{X}_i&#39;\boldsymbol \beta + \varepsilon_i\)</span> para <span class="math inline">\(i = 1, \ldots, n\)</span>, el término de error <span class="math inline">\(\varepsilon_i\)</span> se distribuye como una normal con media cero y varianza constante, <span class="math inline">\(\sigma^2\)</span>:
<span class="math display">\[
\varepsilon_i \sim \mathbf{N}(0, \sigma^2)
\]</span></p>
<p>Asimismo, en genral hemos dicho que, visto como vector, el término de error tiene una distribución de la forma:
<span class="math display">\[
\boldsymbol \varepsilon \sim \mathbf{N}(\mathbf{0}, \sigma^2 \mathbf{I}_n)
\]</span></p>
<p>Obsérvese que la forma de la varianza del término de error: <span class="math inline">\(Var[\boldsymbol \varepsilon | \mathbf X] = \sigma^2 \mathbf{I}_n\)</span>, implica que la distribución de cada una de las <span class="math inline">\(\varepsilon_i\)</span> es independiente, de tel forma que la función de densidad esta dada por:
<span class="math display">\[
f(\varepsilon_i | \boldsymbol \theta) = \frac{1}{\sqrt{2 \pi \sigma^2}}e^{-\frac{1}{2} \frac{(\varepsilon_i - 0)^2}{\sigma^2}}
\]</span></p>
<p>Sustituyendo la definición del término de error obetenemos la siguiente expresión:
<span class="math display">\[\begin{equation}
f(\varepsilon_i | \boldsymbol \theta) = \frac{1}{\sqrt{2 \pi \sigma^2}}e^{-\frac{1}{2} \frac{(\mathbf{X}_i&#39;\boldsymbol \beta - y_i)^2}{\sigma^2}}
\end{equation}\]</span></p>
<p>Donde el vector <span class="math inline">\(\boldsymbol \theta\)</span> se compone de el vector <span class="math inline">\(\boldsymbol \beta\)</span> y <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Por lo tanto, la función de verosimilitud asociada a este caso está dada por la siguiente expresión:
<span class="math display">\[\begin{equation}
L(\boldsymbol{\theta} | \boldsymbol{\varepsilon}) = \prod_{i=1}^n f(\varepsilon_i|\boldsymbol{\theta}) = \prod_{i=1}^n\frac{1}{\sqrt{2 \pi \sigma^2}}e^{-\frac{1}{2} \frac{(\mathbf{X}_i&#39;\boldsymbol \beta - y_i)^2}{\sigma^2}}
\end{equation}\]</span></p>
<p>Esta última ecuación, en su forma logarítmica, se puede expresar como:
<span class="math display">\[\begin{eqnarray}
l(\boldsymbol{\theta} | \boldsymbol{\varepsilon}) 
&amp; = &amp;
\sum_{i=1}^n{ \left[ ln(1) - ln(\sqrt{2 \pi \sigma^2}) - \frac{1}{2} \frac{(y_i - \mathbf{X}_i&#39;\boldsymbol \beta)^2}{\sigma^2} \right]} \nonumber \\
 &amp; = &amp;
\sum_{i=1}^n{ \left[- \frac{1}{2}ln(2 \pi \sigma^2) - \frac{1}{2} \frac{(y_i - \mathbf{X}_i&#39;\boldsymbol \beta)^2}{\sigma^2} \right]} \nonumber \\
 &amp; = &amp;
- \frac{1}{2} \sum_{i=1}^n{ \left[ln(2 \pi \sigma^2) + \frac{(y_i - \mathbf{X}_i&#39;\boldsymbol \beta)^2}{\sigma^2} \right]} \nonumber \\
 &amp; = &amp;
- \frac{1}{2} \left[ \sum_{i=1}^n{ \left[ln(2 \pi \sigma^2) \right]} + \sum_{i=1}^n{ \left[\frac{(y_i - \mathbf{X}_i&#39;\boldsymbol \beta)^2}{\sigma^2} \right]} \right] \nonumber \\\nonumber \\
 &amp; = &amp;
- \frac{1}{2} \left[ n \times ln(2 \pi \sigma^2) + \frac{1}{\sigma^2} \sum_{i=1}^n{ \left[(y_i - \mathbf{X}_i&#39;\boldsymbol \beta)^2\right]} \right] \nonumber \\
 &amp; = &amp;
- \frac{n}{2}ln(2 \pi \sigma^2) - \frac{1}{2 \sigma^2} \boldsymbol \varepsilon&#39; \boldsymbol \varepsilon \nonumber \\
 &amp; = &amp;
- \frac{n}{2}ln(2 \pi \sigma^2) - \frac{1}{2 \sigma^2} (\mathbf{Y} - \mathbf{X} \boldsymbol \beta)&#39;(\mathbf{Y} - \mathbf{X} \boldsymbol \beta) \nonumber \\
 &amp; = &amp;
- \frac{n}{2}ln(2 \pi \sigma^2) - \frac{1}{2 \sigma^2} (\mathbf{Y&#39;Y} - 2\mathbf{Y&#39;X} \boldsymbol \beta + \boldsymbol \beta&#39; \mathbf{X&#39;X} \boldsymbol \beta)
\end{eqnarray}\]</span></p>
<p>Establecida la función de verosimilitud, el siguiente paso consisten en la estimación de los parámetros. Para tal efecto debemos determinar las condiciones de primer orden, quedando de la siguiente forma:
<span class="math display">\[\begin{equation}
\frac{\partial l(\boldsymbol{\theta} | \boldsymbol{\varepsilon})}{\partial \hat{\boldsymbol \beta}} = -2{\mathbf{X&#39;Y}} + 2{\mathbf{X&#39;X}} \hat{\boldsymbol{\beta}} = \mathbf{0}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\frac{\partial l(\boldsymbol{\theta} | \boldsymbol{\varepsilon})}{\partial \hat{\sigma}^2} = - \frac{n}{2 \sigma^2} + \frac{1}{2 (\sigma^2)^2} (\mathbf{Y} - \mathbf{X} \hat{\boldsymbol \beta})&#39;(\mathbf{Y} - \mathbf{X} \hat{\boldsymbol \beta}) = \bf{0}
\end{equation}\]</span></p>
<p>De las dos ecuaciones anteriores podemos deducir las fórmulas de nuestros estimadores de Máxima Verosimilitud:
<span class="math display">\[\begin{equation}
\hat{\boldsymbol{\beta}} = (\mathbf{X&#39;X})^{-1}\mathbf{X&#39;Y}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\hat{\sigma}^2 = \frac{\mathbf{e&#39;e}}{n}
\end{equation}\]</span></p>
<p>El procedimiento de de máxima verosimilitud es el más atractivo de los demás procedimientos de estimación, ya que sus propiedades asíntoticas son que:</p>
<p></p>
<p>Si se asume que la función de densidad conjunta cumple con las condiones de regularidad (que la primer derivada del logarítmo de la función de verosimilitud es continua en todo punto, y que las condiciones de primer órden y segundo órden son conocidas), podemos enunciar el suiguente:</p>
<p>. Bajo condiciones de regularidad, el estimador de máxima verosimilitud posee las siguientes propiedades asíntoticas:</p>

</div>
<div id="métricas-de-bondad-de-ajuste" class="section level2">
<h2><span class="header-section-number">13.3</span> Métricas de bondad de ajuste</h2>
<p>El criterio que dio origen a los estimadores de MCO consiste en el valor m'inimo para la suma del cuadrado de todos los residuales. Esta suma es, por otro lado, una medida de ajuste de la l'inea de regresi'on a los datos. Sin embargo, esta medida puede ser facilmente alterada y, por lo tanto, rescalada por una simple multiplicaci'on de los residuales por cualquier valor. Recordemos que el valor de los residuales esta basado en los valores de <span class="math inline">\(\bf{X}\)</span>, as'i podr'iamos pregntarnos por cuanto de la variaci'on de <span class="math inline">\(\bf{Y}\)</span> es explicada por la varaci'on de <span class="math inline">\(\bf{X}\)</span>.</p>
<p>De la Figura 1 podemos afirmar que la variaci'on total de la variable dependiente <span class="math inline">\(\bf{Y}\)</span> se puede descomponer en dos partes, es decir, la variaci'on total de <span class="math inline">\(\bf{Y}\)</span> se puede expresar como la suma dada por:</p>
<p><span class="math display">\[SST = \sum^{n}_{i=1}{(y_i - \bar{y})^2} \]</span></p>
<p>Dada la deficnici'on de regresi'on tenemos que <span class="math inline">\(\bf{Y} = \bf{X} \hat{\boldsymbol{\beta}} + \bf{e} = \hat{\bf{Y}} + \bf{e}\)</span>. Es decir, <span class="math inline">\(y_i = \hat{y}_i + e_i = {\bf{X}}_i \hat{\boldsymbol{\beta}} + e_i\)</span>. De donde podemos inferir que:</p>
<p><span class="math display">\[ y_i - \bar{y} = \hat{y}_i - \bar{y} + e_i = ({\bf{X}}_i - \bar{\bf{X}}_i )\hat{\boldsymbol{\beta}} + e_i \]</span></p>
<p>Se definimos <span class="math inline">\(\bf{M^0}\)</span> como una matriz saca promedios (con las propiedad de ser idempotente y simetrica) y definida como:</p>
<p><span class="math display">\[
\bf{M^0}= 
\left[ \begin{array}{ccccc}
1 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; \ldots &amp; 1
\end{array} \right] 
-
\left[ \begin{array}{ccccc}
1 &amp; 1 &amp; 1 &amp; \ldots &amp; 1 \\
1 &amp; 1 &amp; 1 &amp; \ldots &amp; 1 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
1 &amp; 1 &amp; 1 &amp; \ldots &amp; 1
\end{array} \right] 
\]</span>
<span class="math display">\[=
\bf{I}_n 
-
\left[ \begin{array}{c}
1  \\
1 \\
\vdots \\
1 
\end{array} \right]  
\left[ \begin{array}{ccccc}
1 &amp; 1 &amp; 1 &amp; \ldots &amp; 1 \\
\end{array} \right] 
=
\bf{I}_n - i&#39;i
\]</span></p>
<p>De tal forma que para cualquier vector o matriz, <span class="math inline">\(\bf{W}\)</span>, sucede que: <span class="math inline">\(\bf{M^0}\bf{W}= \bf{W} - \bar{\bf{W}}\)</span>, por ello le llammos matriz saca promedios. Regresando a nuestra discusi'on, podemos escribir que:</p>
<p><span class="math display">\[\bf{Y} - \bar{\bf{Y}} = \bf{M^0Y} = \bf{M^0X} \hat{\boldsymbol{\beta}} + \bf{M^0e}\]</span></p>
<p>Recordemos que si <span class="math inline">\(\bf{M^0}\)</span> extrae los promedios, estonces <span class="math inline">\(\bf{M^0e} = e\)</span>. As'i podemos verificar que el producto <span class="math inline">\(\bf{Y&#39;M^0}\bf{M^0Y}\)</span> es igual a:</p>
<p><span class="math display">\[SST = ({\bf{Y}} - \bar{\bf{Y}})&#39;({\bf{Y}} - \bar{\bf{Y}}) = \sum^{n}_{i=1}{(y_i - {\bar{y}})^2} = \bf{Y&#39;M^0Y}\]</span></p>
<p><span class="math display">\[\bf{Y&#39;M^0Y} = Y&#39;(\bf{M^0X} \hat{\boldsymbol{\beta}} + \bf{M^0e}) = (\bf{X} \hat{\boldsymbol{\beta}} + \bf{e})&#39;(\bf{M^0X} \hat{\boldsymbol{\beta}} + \bf{M^0e})\]</span>
<span class="math display">\[= (\hat{\boldsymbol{\beta}}&#39; \bf{X&#39;} + \bf{e&#39;})(\bf{M^0X} \hat{\boldsymbol{\beta}} + \bf{M^0e}) \]</span>
<span class="math display">\[= \hat{\boldsymbol{\beta}}&#39; \bf{X&#39;}\bf{M^0X} \hat{\boldsymbol{\beta}} + \hat{\boldsymbol{\beta}}&#39; \bf{X&#39;}\bf{M^0e} + \bf{e&#39;}\bf{M^0X} \hat{\boldsymbol{\beta}} + \bf{e&#39;}\bf{M^0e}\]</span></p>
<p>Finlamente, como recordaran de clases pasadas, dijimos que s'olo cuando nuestra regresi'on inclui constante la suma de residuales <span class="math inline">\(\sum^{n}_{i=1}{e_i} = 0\)</span>. De esta forma, el promedio de residuales <span class="math inline">\((\sum^{n}_{i=1}{e_i})/n = 0\)</span>. Es decir, que nuestra matriz saca promedio multiplicada por el vector de residuales es igual a <span class="math inline">\(\bf{M^0e} = e - \bar{e} = e\)</span>. As'i:</p>
<p><span class="math display">\[\bf{Y&#39;M^0Y} = \hat{\boldsymbol{\beta}}&#39; \bf{X&#39;}\bf{M^0X} \hat{\boldsymbol{\beta}} + \hat{\boldsymbol{\beta}}&#39; \bf{X&#39;}\bf{e} + \bf{e&#39;}\bf{X} \hat{\boldsymbol{\beta}} + \bf{e&#39;}\bf{e}\]</span></p>
<p>Por otro lado, sabemos que la soluci'on por el m'etodo de MCO garantiza que el producto de <span class="math inline">\(\bf{X&#39;e} = \bf{e&#39;X} = 0\)</span>.</p>
<p><span class="math display">\[SST = \bf{Y&#39;M^0Y} = \hat{\boldsymbol{\beta}}&#39; \bf{X&#39;}\bf{M^0X} \hat{\boldsymbol{\beta}} + \bf{e&#39;}\bf{e}\]</span>
<span class="math display">\[SST = SSR + SSE\]</span></p>
<p>O en palabras, la variavilidad total de <span class="math inline">\(\bf{Y}\)</span> se puede descomponer en dos: la variavilidad originada por la regresi'on y la variavilidad que no puede ser explicada, es decir, la del t'ermino de error.</p>
<p>Dicho esto, porponemos el siguiente coeficiente de bondad de ajuste a los datos, el cual suele concerse como <span class="math inline">\(R^2\)</span>:</p>
<p><span class="math display">\[R^2 = \frac{SSR}{SST} = \frac{SST - SSE}{SST} = 1 - \frac{SSE}{SST}\]</span></p>
<p>En f'ormula:</p>
<p><span class="math display">\[R^2 = 1 - \frac{\bf{e&#39;e}}{\bf{Y&#39;M^0Y}}\]</span></p>
<p>Por 'ultimo el <span class="math inline">\(R^2\)</span> ajustado solo es:</p>
<p><span class="math display">\[R^2 = 1 - \frac{{\bf{e&#39;e}}/(n-K)}{{\bf{Y&#39;M^0Y}}/(n-1)}\]</span></p>
</div>
<div id="pruebas-de-hipótesis" class="section level2">
<h2><span class="header-section-number">13.4</span> Pruebas de Hipótesis</h2>
<p>El An'alisis de Regresi'on se suele usar con mucha frecuencia para los siguientes prop'ositos: la estimaci'on y predicci'on, y para probar alg'un tipo de hip'otesis. La estimaci'on y predicci'on se analizar'a con mayor detalle al final de esta clase y el sesiones futuras. Por lo que respecta a las pruebas de hip'otesis estableceremos los siguiente.</p>
<p>Recordemos que nuestro modelo general de regresi'on est'a dado por la siguiente expresi'on:</p>
<p><span class="math display">\[{\bf Y} = {\bf X}{\boldsymbol \beta} + {\boldsymbol \varepsilon}\]</span></p>
<p>Ahora consideremos un ejemplo, supongamos que desea plantear una regresi'on del tipo logar'itmica con el objeto de determinar la demanda de tabaco. As'i, establece la siguiente relaci'on:</p>
<p><span class="math display">\[ln(Q_{Tabaco}) = \beta_0 + \beta_1 ln(P_{Tabaco}) + \beta_2 ln(P_{Alcohol}) + \beta_3 ln(Ingreso) + \varepsilon\]</span></p>
<p>Donde <span class="math inline">\(Q_{Tabaco}\)</span> es la cantidad de tabaco demandada, y <span class="math inline">\(P_{Tabaco}\)</span> y <span class="math inline">\(P_{Alcohol}\)</span> son el precio del tabaco y del aocohol, respectivamente. Suponga, adicionalmente, que sospecha que el tabaco y el alcohol guardan una relaci'on de complementariedad, por lo que espera que los param'etros asociados a las variables de precios de ambos tengan el mismo signo (-). Asimismo, suponga que estas variables son las 'unicas relevantes para este caso y el resto de la informaci'on es no observable o no medible.</p>
<p>Supongamos, quiz'a de forma absurda, que el tabaco y el alcohol exhiben una el'asticidad unitaria, por lo que decide plantear la siguiente hip'otesis:</p>
<p><span class="math display">\[H_0: \beta_1 = 1 \medspace y \medspace \beta_2 = 1\]</span></p>
<p><span class="math display">\[H_1: No \medspace H_0\]</span></p>
<p>La hip'otesis nula es equivalente a escribir el siguiente sistema de ecuaciones:</p>
<p><span class="math display">\[0 \beta_0 + 1 \beta_1 + 0 \beta_2 + 0 \beta_3 = 1\]</span>
<span class="math display">\[0 \beta_0 + 0 \beta_1 + 1 \beta_2 + 0 \beta_3 = 1\]</span></p>
<p>El cual podemos escribir como:</p>
<p><span class="math display">\[
\left(
\begin{array}{c c c c}
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
\end{array}
\right)
\left(
\begin{array}{c}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\beta_3 \\
\end{array}
\right)
=
\left(
\begin{array}{c}
1 \\
1 \\
\end{array}
\right)
\]</span></p>
<p>En forma reducida:</p>
<p><span class="math display">\[{\bf R} {\boldsymbol \beta} = q\]</span></p>
<p>Con lo cual la hip'otesis original se puede escribir como:</p>
<p><span class="math display">\[H_0: {\bf R} {\boldsymbol \beta} = q\]</span></p>
<p><span class="math display">\[H_1: {\bf R} {\boldsymbol \beta} \ne q\]</span></p>
<p>Observemos que podemos afirmar que la hip'otesis tiene dos restricciones. Regresando a nuestro caso general:</p>
<p><span class="math display">\[{\bf Y} = {\bf X}{\boldsymbol \beta} + {\boldsymbol \varepsilon}\]</span></p>
<p>Por analog'ia podr'iamos escribir un conjunto de ``J’’ restricciones como:</p>
<p><span class="math display">\[r_{10} \beta_0 + r_{11} \beta_1 + \ldots + r_{1K} \beta_K = q_1\]</span>
<span class="math display">\[r_{20} \beta_0 + r_{21} \beta_1 + \ldots + r_{2K} \beta_K = q_2\]</span>
<span class="math display">\[\vdots\]</span>
<span class="math display">\[r_{J0} \beta_0 + r_{J1} \beta_1 + \ldots + r_{JK} \beta_K = q_J\]</span></p>
<p>Sistema que se puede escribir en forma matricial como:</p>
<p><span class="math display">\[
\left(
\begin{array}{c c c c}
r_{10} &amp; r_{11} &amp; \ldots &amp; r_{1K} \\
r_{20} &amp; r_{21} &amp; \ldots &amp; r_{2K} \\
\vdots &amp; \vdots &amp; \ldots &amp; \vdots \\
r_{J0} &amp; r_{J1} &amp; \ldots &amp; r_{JK} \\
\end{array}
\right)
\left(
\begin{array}{c}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_3 \\
\end{array}
\right)
=
\left(
\begin{array}{c}
q_1 \\
q_2 \\
\vdots \\
q_J \\
\end{array}
\right)
\]</span></p>
<p>De esta forma podemos, finalmente, escribir una hip'otesis general:</p>
<p><span class="math display">\[H_0: {\bf R} {\boldsymbol \beta} = q\]</span></p>
<p><span class="math display">\[H_1: No \medspace H_0\]</span></p>
<p>Dicho lol anterior, el restante argumento versa sobre c'omo hacer uso de estas restricciones conjuntas.</p>

<p>Cuando deseamos evaluar una hip'otesis con m'as de una restricci'on se debe ocupar la prueba F. La cual se puede escribir como:</p>
<p><span class="math display">\[F_{[J, n-K]} = \frac{({\bf R}{\boldsymbol \beta} - {\bf q})&#39; [s^2 {\bf R} ({\bf X&#39;X})^{-1} {\bf R}&#39;] ({\bf R}{\boldsymbol \beta} - {\bf q})}{J}\]</span></p>
<p>Esta estad'istica se distribuye como una F de Fisher con <span class="math inline">\(J\)</span> y <span class="math inline">\(n-K\)</span> grados de libertad.</p>

<p>Cuando deseamos evaluar una hip'otesis con solo una restricci'on se debe ocupar la prueba t. La cual se puede escribir como:</p>
<p><span class="math display">\[t_{[n-K, \alpha/2]} = ({\bf R}{\boldsymbol \beta} - {\bf q})&#39; [s^2 {\bf R} ({\bf X&#39;X})^{-1} {\bf R}&#39;] ({\bf R}{\boldsymbol \beta} - {\bf q})\]</span></p>
<p>Esta estad'istica se distribuye como una t con <span class="math inline">\(n-K\)</span> grados de libertad.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="otros-modelos-de-series-de-tiempo-no-lineales.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Notas de Clase.pdf", "Notas de Clase.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
